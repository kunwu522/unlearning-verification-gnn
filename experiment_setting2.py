'''
Experiment of checking the necessity of adversarial edges generated by Nettack.

Kun Wu
Stevens Institute of Technology
'''
import os
import time
import math
import copy
import random
from itertools import combinations
from collections import defaultdict
from tqdm import tqdm
import numpy as np
import pandas as pd
import torch
from torch_geometric.utils import to_undirected, k_hop_subgraph
import argument
import data_loader
from model.gcn import GNN
from nettack_adapter import adapte as ntk
import utils


def sample_node_tokens(args, data, candidate_nodes, posterior):
    if args.sampler == 'random':
        # result = random.sample(candidate_nodes, min(args.num_target_nodes, len(candidate_nodes)))
        result = []
        while len(result) < args.num_target_nodes:
            node = random.choice(candidate_nodes)
            if node in result:
                continue
            degree = data.degree(node)
            if degree > 1 and degree < 6:
                result.append(node)
    else:
        near_boundary_nodes = {}
        for idx, p in enumerate(posterior):
            # if utils.near_boundary(z, args.k):
            near_boundary_nodes[candidate_nodes[idx]] = utils.boundary_score(p)
        sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1], reverse=args.sampler=='distance')}

        i = 0
        result = []
        while len(result) < args.num_target_nodes:
            n = list(sorted_boundary_nodes.keys())[i]
            degree = data.degree(n)
            if degree >= 2 and degree <=5:
                result.append(n)
            i += 1
        print(f'Found {args.num_target_nodes} target nodes in top {i} {args.sampler} nodes.')
        # result = list(sorted_boundary_nodes.keys())[:min(len(candidate_nodes), args.num_target_nodes)]
    return result


def sample_edge_tokens(args, data, target_node, surrogate_model=None, prediction=None, label=None):
    E_r, E_remain = utils.sample_edges('random', data.edge_index, n_perturbations)
    E_r = list(map(tuple, E_r))

    if args.edge_sampler == 'Nettack':
        assert surrogate_model is not None, f'Invalid input, did not find surrogate model.'
        assert prediction is not None, f'Invalid input, did not find prediction.'
        assert label is not None, f'Invalid input, did not find label.'

        n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
        nettack = ntk(surrogate_model, data, target_node, prediction, label)
        nettack.reset()
        nettack.attack_surrogate(n_perturbations, perturb_structure=True, perturb_features=False, direct=True, n_influencers=1)
        # print('Perturbations:', nettack.structure_perturbations)
        # adv_nodes = np.array(nettack.structure_perturbations).T[1]
        E_t = nettack.structure_perturbations
        # _edge_index = to_undirected(torch.tensor(nettack.structure_perturbations).t())
    elif args.edge_sampler == 'independent':
        n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
        # find uninfected nodes and randomly generate edges
        k_hop_nodes, _, _, _ = k_hop_subgraph(target_node, 2, edge_index=data.edge_index)
        nodes = torch.arange(data.num_nodes)
        uninfected_nodes = nodes[~torch.isin(nodes, k_hop_nodes)]
        uninfected_edges, adv_nodes = [], []
        while len(uninfected_edges) < n_perturbations:
            u, v = random.sample(uninfected_nodes.tolist(), 2)
            if (u, v) in E_r or (v, u) in E_r:  # Under setting 2, independent edges should not in E_R
                continue

            comp = torch.isin(data.edge_index, torch.tensor([u, v]))
            if torch.sum(comp[0] & comp[1]) == 0:
                uninfected_edges.append([u, v])
                adv_nodes.extend([u, v]) 
        E_t = uninfected_edges
        # _edge_index = to_undirected(torch.tensor(uninfected_edges).t())
    return E_t, E_r

def calculate_q(args):
    device = utils.get_device(args)
    print(device)

    result = defaultdict(list)
    for t in range(args.num_trials):
        data = data_loader.load(args)

        surrogate_model = GNN(args, data.num_features, data.num_classes, surrogate=False)
        surrogate_model.train(data, device)

        candidates = data.test_set.nodes.tolist()
        _, _, surrogate_post = surrogate_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        target_nodes = sample_node_tokens(args, data, candidates, surrogate_post)
        predictions, labels, posteriors = surrogate_model.predict(data, device, target_nodes=target_nodes, return_posterior=True)

        for target_node, prediction, label, post in tqdm(zip(target_nodes, predictions, labels, posteriors), total=len(target_nodes), desc=f'Trial {t}'):
            # print(uninfected_edge_index)
            E_t, E_r = sample_edge_tokens(args, data, target_node, surrogate_model, prediction, label)
            
            adv_data = copy.deepcopy(data)
            adv_data.add_edges(E_t)
            adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False)
            adv_model.train(adv_data, device)
            adv_pred, _, adv_post = adv_model.predict(adv_data, device, target_nodes=[target_node], return_posterior=True)

            retrain_data = copy.deepcopy(data)
            retrain_data.remove_edges(E_r)
            clean_model = GNN(args, retrain_data.num_features, retrain_data.num_classes, surrogate=False)
            clean_model.train(retrain_data, device)
            clean_pred, _, clean_post = clean_model.predict(retrain_data, device, target_nodes=[target_node], return_posterior=True)

            num_of_c = sum([math.comb(n_perturbations, p) for p in range(1, n_perturbations)])
            num_of_flip = 0
            statistic = defaultdict(int)
            for length in range(1, len(uninfected_edges)):
                for _edges in combinations(uninfected_edges, length):
                    sub_adv_nodes = [v for _, v in _edges]
                    sub_adv_data = copy.deepcopy(adv_data)
                    sub_adv_data.remove_edges(_edges)
                    # print(adv_data.edge_index.size(1), '-', len(_edges), '=', sub_adv_data.edge_index.size(1))
                    sub_adv_model = GNN(args, sub_adv_data.num_features, sub_adv_data.num_classes, surrogate=False)
                    sub_adv_model.train(sub_adv_data, device)
                    sub_adv_pred, _,  sub_adv_post = sub_adv_model.predict(sub_adv_data, device, target_nodes=[target_node], return_posterior=True)
                    if adv_pred[0] != sub_adv_pred[0]:
                        num_of_flip += 1
                        statistic[len(_edges)] += 1

                    # print('#' * 15, f'Target node: {target_node}', '#' * 15)
                    # print('            ground truth:', label)
                    # print('    surrogate prediction:', prediction)
                    # print('  adversarial prediction:', adv_pred[0])
                    # print('        clean prediction:', clean_pred[0])
                    # print('      sub adv prediction:', sub_adv_pred[0])
                    # print('    surrogate post order:', np.argsort(post.squeeze())[::-1])
                    # print('        clean post order:', np.argsort(clean_post.squeeze())[::-1])
                    # print('  adversarial post order:', np.argsort(adv_post.squeeze())[::-1])
                    # print('      sub adv post order:', np.argsort(sub_adv_post.squeeze())[::-1])
                    # print('       adversarial nodes:', adv_nodes)
                    # print('adversarial nodes labels:', data.y[adv_nodes].tolist())
                    # print('       removed adv nodes:', sub_adv_nodes)
                    # print('removed adv nodes labels:', data.y[sub_adv_nodes].tolist())
                    # print('#' * 50)

            result['target node'].append(target_node)
            result['# of adv edges'].append(n_perturbations)
            result['# of combinations'].append(num_of_c)
            result['# of flip'].append(num_of_flip)
            result['surrogate prediction'].append(prediction)
            result['adv prediction'].append(adv_pred[0])
            result['clean prediction'].append(clean_pred[0])
            result['ground truth'].append(label)
            result['detail'].append(statistic)
    
        _df = pd.DataFrame(result)
        _df = pd.DataFrame(result)
        _q = np.sum(_df['# of flip'] != 0) /  len(_df)
        print('====> The q =', _q)

    df = pd.DataFrame(result)
    q = np.sum(df['# of flip'] > 0) / len(df)
    print('The overall q: ', q)


    result_path = os.path.join('result', f'setting2_{args.dataset}_{args.sampler}{args.num_target_nodes}')
    timestamp = int(time.time())
    result_path += f'_{timestamp}.csv'
    df.to_csv(result_path)
    print('Finished, save the result to', result_path)



if __name__ == '__main__':
    parser = argument.load_parser()
    parser.add_argument('--num-target-nodes', dest='num_target_nodes', type=int, default=10)
    parser.add_argument('--node-sampler', dest='node_sampler', type=str, default='random', 
                        help='How to sample target nodes, distance|boundary')
    parser.add_argument('--edge-sampler', dest='edge_sampler', type=str, default='nettack')
    args = parser.parse_args()
    calculate_q(args)

