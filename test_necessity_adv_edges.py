'''
Experiment of checking the necessity of adversarial edges generated by Nettack.

hiding for anonymity
'''
import os
import time
import math
import copy
import random
from itertools import combinations
from collections import defaultdict
from tqdm import tqdm
import numpy as np
import pandas as pd
import torch
from torch_geometric.utils import to_undirected, sort_edge_index
import argument
import data_loader
from model.gcn import GNN
from nettack_adapter import adapte as ntk
import utils


def sample_target_nodes(args, data, candidate_nodes, posterior):
    if args.sampler == 'random':
        # result = random.sample(candidate_nodes, min(args.num_target_nodes, len(candidate_nodes)))
        result = []
        while len(result) < args.num_target_nodes:
            node = random.choice(candidate_nodes)
            if node in result:
                continue
            degree = data.degree(node)
            if degree > 1 and degree < 6:
                result.append(node)
    else:
        near_boundary_nodes = {}
        for idx, p in enumerate(posterior):
            # if utils.near_boundary(z, args.k):
            near_boundary_nodes[candidate_nodes[idx]] = utils.boundary_score(p)
        sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1], reverse=args.sampler=='distance')}

        i = 0
        result = []
        while len(result) < args.num_target_nodes:
            n = list(sorted_boundary_nodes.keys())[i]
            degree = data.degree(n)
            if degree >= 2 and degree <=5:
                result.append(n)
            i += 1
        print(f'Found {args.num_target_nodes} target nodes in top {i} {args.sampler} nodes.')
        # result = list(sorted_boundary_nodes.keys())[:min(len(candidate_nodes), args.num_target_nodes)]
    return result

def check_necessity(args):
    device = utils.get_device(args)
    print(device)

    result = defaultdict(list)
    for t in range(args.num_trials):
        data = data_loader.load(args)

        surrogate_model = GNN(args, data.num_features, data.num_classes, surrogate=True)
        surrogate_model.train(data, device)

        candidates = data.test_set.nodes.tolist()
        _, _, surrogate_post = surrogate_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        target_nodes = sample_target_nodes(args, data, candidates, surrogate_post)
        predictions, labels, posteriors = surrogate_model.predict(data, device, target_nodes=target_nodes, return_posterior=True)
        degree_dist = defaultdict(int)
        for n in target_nodes:
            degree_dist[data.degree(n)] += 1
        print('degree distribution', degree_dist)

        count = 0
        for target_node, prediction, label, post in tqdm(zip(target_nodes, predictions, labels, posteriors), total=len(target_nodes), desc=f'Trial {t}'):
            n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
            nettack = ntk(surrogate_model, data, target_node, prediction, label)
            nettack.reset()
            nettack.attack_surrogate(n_perturbations, perturb_structure=True, perturb_features=False, direct=True, n_influencers=1)
            # print('Perturbations:', nettack.structure_perturbations)
            adv_nodes = np.array(nettack.structure_perturbations).T[1]

            adv_edge_index = to_undirected(torch.tensor(nettack.structure_perturbations).t())
            adv_data = copy.deepcopy(data)
            adv_data.add_edges(adv_edge_index)
            adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False)
            adv_model.train(adv_data, device)
            adv_pred, _, adv_post = adv_model.predict(adv_data, device, target_nodes=[target_node], return_posterior=True)

            clean_model = GNN(args, data.num_features, data.num_classes, surrogate=False)
            clean_model.train(data, device)
            clean_pred, _, clean_post = clean_model.predict(data, device, target_nodes=[target_node], return_posterior=True)

            num_of_c = sum([math.comb(n_perturbations, p) for p in range(1, n_perturbations)])
            num_of_flip = 0
            statistic = defaultdict(int)
            for length in range(1, len(nettack.structure_perturbations)):
                for _edges in combinations(nettack.structure_perturbations, length):
                    sub_adv_nodes = [v for _, v in _edges]
                    sub_adv_data = copy.deepcopy(adv_data)
                    sub_adv_data.remove_edges(_edges)
                    # print(adv_data.edge_index.size(1), '-', len(_edges), '=', sub_adv_data.edge_index.size(1))
                    sub_adv_model = GNN(args, sub_adv_data.num_features, sub_adv_data.num_classes, surrogate=False)
                    sub_adv_model.train(sub_adv_data, device)
                    sub_adv_pred, _,  sub_adv_post = sub_adv_model.predict(sub_adv_data, device, target_nodes=[target_node], return_posterior=True)
                    if adv_pred[0] != sub_adv_pred[0]:
                        num_of_flip += 1
                        statistic[len(_edges)] += 1

                    # print('#' * 15, f'Target node: {target_node}', '#' * 15)
                    # print('            ground truth:', label)
                    # print('    surrogate prediction:', prediction)
                    # print('  adversarial prediction:', adv_pred[0])
                    # print('        clean prediction:', clean_pred[0])
                    # print('      sub adv prediction:', sub_adv_pred[0])
                    # print('    surrogate post order:', np.argsort(post.squeeze())[::-1])
                    # print('        clean post order:', np.argsort(clean_post.squeeze())[::-1])
                    # print('  adversarial post order:', np.argsort(adv_post.squeeze())[::-1])
                    # print('      sub adv post order:', np.argsort(sub_adv_post.squeeze())[::-1])
                    # print('       adversarial nodes:', adv_nodes)
                    # print('adversarial nodes labels:', data.y[adv_nodes].tolist())
                    # print('       removed adv nodes:', sub_adv_nodes)
                    # print('removed adv nodes labels:', data.y[sub_adv_nodes].tolist())
                    # print('#' * 50)
            if num_of_flip > 0:
                count += 1
            result['target node'].append(target_node)
            result['# of adv edges'].append(n_perturbations)
            result['# of combinations'].append(num_of_c)
            result['# of flip'].append(num_of_flip)
            result['surrogate prediction'].append(prediction)
            result['adv prediction'].append(adv_pred[0])
            result['clean prediction'].append(clean_pred[0])
            result['ground truth'].append(label)
            result['detail'].append(statistic)

        print(f'At trial {t}, q:', count / args.num_target_nodes)

    df = pd.DataFrame(result)
    q = np.sum(df['# of flip'] > 0) / len(df)
    print('The overall q: ', q)


    result_path = os.path.join('result', f'necessity_{args.dataset}_{args.sampler}{args.num_target_nodes}')
    timestamp = int(time.time())
    result_path += f'_{timestamp}.csv'
    df.to_csv(result_path)
    print('Finished, save the result to', result_path)



if __name__ == '__main__':
    parser = argument.load_parser()
    parser.add_argument('--num-target-nodes', dest='num_target_nodes', type=int, default=10)
    parser.add_argument('--sampler', type=str, default='distance', help='How to sample target nodes, distance|boundary')
    args = parser.parse_args()
    check_necessity(args)

