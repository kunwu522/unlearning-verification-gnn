'''
Experiment of checking the necessity of adversarial edges generated by Nettack.

Kun Wu
Stevens Institute of Technology
'''
import os
import time
import math
import copy
import random
import logging
from itertools import combinations
from collections import defaultdict
from tqdm import tqdm
import numpy as np
import scipy.sparse as sp
import pandas as pd
import torch
from torch_geometric import seed_everything
from torch_geometric.utils import to_undirected, sort_edge_index, k_hop_subgraph
from deeprobust.graph.defense import GCN
from deeprobust.graph.targeted_attack import IGAttack

import argument
import data_loader
from model.gcn import GNN
from nettack_adapter import adapte as ntk
import utils

# logging.basicConfig(encoding='utf-8', level=logging.INFO,
#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# def homophily(nodes, adj, labels):
#     num_nodes = len(nodes)
#     num_edges = torch.sum(adj) / 2

#     s = 0
#     for i in range(num_nodes):
#         for j in range(i+1, num_nodes):
#             i_deg = torch.sum(adj[i])
#             j_deg = torch.sum(adj[j])
#             s += adj[i, j] - (i_deg * j_deg) / (2 * num_edges) if labels[i] == labels[j] else 0

#     return s / (2 * num_edges)

def sample_node_tokens(args, data, candidate_nodes, posterior):
    if args.node_sampler == 'random':
        # result = random.sample(candidate_nodes, min(args.num_target_nodes, len(candidate_nodes)))
        result = []
        while len(result) < args.num_target_nodes:
            node = random.choice(candidate_nodes)
            if node in result:
                continue
            # degree = data.degree(node)
            # if degree > 1 and degree < 6:
            #     result.append(node)
            result.append(node)

    elif args.node_sampler == 'purity':
        _candidates = []
        for v in candidate_nodes:
            subgraph = k_hop_subgraph(v, 1, data.edge_index)[0]
            if torch.unique(data.y[subgraph]).size(0) == 1:
                _candidates.append(v)

        result = []
        while len(result) < args.num_target_nodes:
            v = random.choice(_candidates)
            if v in result:
                continue
            degree = data.degree(v)
            if degree > 1 and degree < 10:
                result.append(v)

    elif args.node_sampler == 'homophily':
        node2homophily = {}
        for v in candidate_nodes:
            subgraph, _edge_index, _, _ = k_hop_subgraph(v, 1, data.edge_index, relabel_nodes=True)
            node2homophily[v] = utils.homophily_entropy(data.num_classes, data.y[subgraph])
            # adj = torch.zeros(len(subgraph), len(subgraph))
            # adj[_edge_index.tolist()] = 1

            # node2homophily[v] = utils.homophily(subgraph, adj, data.y[subgraph])

        # sorted_node2homophily = {k: v for k,v in sorted(node2homophily.items(), key=lambda item: item[1])}
        sorted_node2homophily = {k: v for k,v in sorted(node2homophily.items(), key=lambda item: item[1], reverse=True)}
        _candidate_tokens = list(sorted_node2homophily.keys())
        if args.num_target_nodes == 1:
            v = random.choice(_candidate_tokens[:10])
            result = [v]
        else:
            i = 0
            result = _candidate_tokens[:args.num_target_nodes]
    else:
        if posterior is None:
            surrogate = GNN(args, data.num_features, data.num_classes, surrogate=True)
            surrogate.train(data, utils.get_device(args))
            _, _, posterior = surrogate.predict(data, utils.get_device(args), target_nodes=candidate_nodes, return_posterior=True)

        near_boundary_nodes = {}
        for idx, p in enumerate(posterior):
            # if utils.near_boundary(z, args.k):
            near_boundary_nodes[candidate_nodes[idx]] = utils.boundary_score(p)
        sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1], reverse=args.node_sampler=='distance')}
        sorted_values = np.array(list(sorted_boundary_nodes.values()))
        theta_idx = np.where(sorted_values > args.theta)[0][0]
        _candidate_tokens = list(sorted_boundary_nodes.keys())[theta_idx:]

        # if args.theta == 0:
        #     sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1], reverse=args.node_sampler=='distance')}
        #     _candidate_tokens = list(sorted_boundary_nodes.keys())
        # else:
        #     sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1], reverse=False)}
        #     sorted_values = np.array(list(sorted_boundary_nodes.values()))
        #     valid_indices = np.where(sorted_values < args.theta)[0][::-1]
        #     _candidate_tokens = np.array(list(sorted_boundary_nodes.keys()))[valid_indices]

        if args.num_target_nodes == 1:
            # while True:
            #     v = random.choice(_candidate_tokens[:10])
                # degree = data.degree(v)
                # if degree >= 2 and degree <=5:
                #     break
            
            v = random.choice(_candidate_tokens[:10])
            result = [v]
        else:
            i = 0
            result = []
            while len(result) < args.num_target_nodes:
                n = _candidate_tokens[i]
                # degree = data.degree(n)
                # if degree >= 2 and degree <=5:
                #     result.append(n)
                result.append(n)
                i += 1
        # result = list(sorted_boundary_nodes.keys())[:min(len(candidate_nodes), args.num_target_nodes)]
    print(f'Found {args.num_target_nodes} {args.node_sampler} target nodes nodes.')
    return result

def sample_edge_tokens(args, data, target_node, surrogate_model=None, prediction=None, label=None, second_best=None):
    if args.edge_sampler == 'nettack':
        assert surrogate_model is not None, f'Invalid input, did not find surrogate model.'
        assert prediction is not None, f'Invalid input, did not find prediction.'
        assert label is not None, f'Invalid input, did not find label.'
        # n_perturbations = random.randint(data.degree(target_node), data.degree(target_node) * 2)
        n_perturbations = int(data.degree(target_node)) + 1 if data.degree(target_node) > 1 else 5
        nettack = ntk(surrogate_model, data, target_node, prediction, second_best=second_best, epsilon=args.epsilon)
        nettack.reset()
        nettack.attack_surrogate(n_perturbations, perturb_structure=True, perturb_features=False, direct=True, n_influencers=1)
        E_t = nettack.structure_perturbations
        # _edge_index = to_undirected(torch.tensor(nettack.structure_perturbations).t())
    if args.edge_sampler == 'ig':
        device = utils.get_device(args)
        surrogate = GCN(nfeat=data.num_features, nclass=data.num_classes, nhid=16, with_relu=True, with_bias=False, device=device).to(device)
        # contruct adjacency matrix
        row = data.edge_index.numpy()[0]
        col = data.edge_index.numpy()[1]
        value = np.ones((len(row)))
        adj = sp.csr_matrix((value, (row, col)), shape=(data.num_nodes, data.num_nodes))
        surrogate.fit(data.x, adj, data.y, data.train_set.nodes.tolist(), data.valid_set.nodes.tolist())
        # contruct features sparse
        row, col = np.where(data.x.numpy() == 1)
        value = np.ones((len(row)))
        x = sp.csr_matrix((value, (row, col)), shape=data.x.shape)
        labels = data.y.numpy()

        attacker = IGAttack(surrogate, data.num_nodes, attack_structure=True, attack_features=False, device=device).to(device)
        n_perturbations = data.degree(target_node) if data.degree(target_node) > 1 else 2
        attacker.attack(x, adj, labels, data.train_set.nodes.tolist(), target_node, n_perturbations=n_perturbations, steps=10)
        print()
        print('# of add edges', np.sum(attacker.modified_adj - adj == 1))
        print('# of removed edges', np.sum(attacker.modified_adj - adj == -1))
        perturbed_edges = np.array(np.where(attacker.modified_adj != adj.toarray()))
        perturbed_edges = perturbed_edges[:, perturbed_edges[0] < perturbed_edges[1]]
        # print(perturbed_edges)
        E_t = list(map(tuple, perturbed_edges.T.tolist()))
        # exit(0)
    elif args.edge_sampler == 'independent':
        # n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
        n_perturbations = random.randint(data.degree(target_node), data.degree(target_node) * 2)
        # find uninfected nodes and randomly generate edges
        k_hop_nodes, _, _, _ = k_hop_subgraph(target_node, 2, edge_index=data.edge_index)
        nodes = torch.arange(data.num_nodes)
        uninfected_nodes = nodes[~torch.isin(nodes, k_hop_nodes)]
        uninfected_edges, adv_nodes = [], []
        while len(uninfected_edges) < n_perturbations:
            u, v = random.sample(uninfected_nodes.tolist(), 2)
            comp = torch.isin(data.edge_index, torch.tensor([u, v]))
            if torch.sum(comp[0] & comp[1]) == 0:
                uninfected_edges.append([u, v])
                adv_nodes.extend([u, v]) 
        E_t = uninfected_edges
        # _edge_index = to_undirected(torch.tensor(uninfected_edges).t())
    return E_t


def k_hop_subgraph_label_distribution(v, k, edge_index, num_classes, labels):
    subgraph = k_hop_subgraph(v, k, edge_index)[0]
    label_dist = np.zeros((num_classes))
    for value, count in zip(*np.unique(labels[subgraph].numpy(), return_counts=True)):
        label_dist[value] = count
    return label_dist


def estimate_q(args):
    device = utils.get_device(args)
    result = defaultdict(list)
    data = data_loader.load(args)

    target_nodes = sample_node_tokens(args, data, data.train_set.nodes.tolist(), None)
    # if 1936 not in target_nodes:
        # target_nodes.insert(0, 1936)
    if 427 not in target_nodes:
        target_nodes.insert(0, 427)

    _surrogate_model = GNN(args, data.num_features, data.num_classes, surrogate=False)
    _surrogate_model.train(data, device)

    # for _ in range(5):
    #     seed_everything(522)
    #     test_model = GNN(args, data.num_features, data.num_classes, surrogate=False)
    #     test_model.train(data, device)
    #     test_pred, _, test_post = test_model.predict(data, device, target_nodes=[1936], return_posterior=True)
    #     result = test_model.evaluate(data, device)
    #     print(test_post, result)
    # exit(0)

    # for _ in tqdm(range(args.num_target_nodes), desc='calculating q'):
    for target_node in tqdm(target_nodes, desc='calculating q'):
        print(f'Attack {target_node} (degree: {data.degree(target_node)})')
        # seed_everything(522)
        # for _ in range(args.num_trials):
        clean_model = GNN(args, data.num_features, data.num_classes, surrogate=False)
        clean_model.train(data, device)
        clean_pred, label, clean_post, clean_logit = clean_model.predict(data, device, target_nodes=[target_node], return_posterior=True, return_logit=True)
        clean_post = clean_post.squeeze()

        pred, _ = _surrogate_model.predict(data, device, target_nodes=[target_node])
        clean_logit = clean_logit.squeeze()
        c_old = clean_logit[clean_pred]
        c = np.max(np.delete(clean_logit, clean_pred))
        original_score = c_old - c
        print('original_score =', original_score, ', second_best:', np.argsort(clean_post)[-2])

        # k_subgraph = k_hop_subgraph(target_node, 2, data.edge_index)[0]
        # k_label_dist = np.zeros((data.num_classes))
        # for value, count in zip(*np.unique(data.y[k_subgraph].numpy(), return_counts=True)):
        #     k_label_dist[value] = count
        k_label_dist = k_hop_subgraph_label_distribution(target_node, 2, data.edge_index, data.num_classes, data.y)
        label_dist = k_hop_subgraph_label_distribution(target_node, 1, data.edge_index, data.num_classes, data.y)

        E_t = sample_edge_tokens(args, data, target_node, _surrogate_model, clean_pred, label)
        print(' # of E_t', len(E_t))

        print('-' * 20)
        print(' ' * 2 + 'Clean posterior:', clean_post.tolist())
        print(' ' * 2, 'label:', label[0])
        print(' ' * 2, 'clean prediction:', clean_pred[0])
        print(' ' * 2, '1-hop label distribution:', label_dist)
        print(' ' * 2, 'k-hop label distribution:', k_label_dist)
        print('-' * 20)

        for i in range(1, len(E_t)+1):
            # seed_everything(522)
            _E_t = E_t[:i]
            _edge_index_t = to_undirected(torch.tensor(_E_t).t())
            adv_data = copy.deepcopy(data)
            adv_data.add_edges(_edge_index_t)
 
            _, _, _clean_logit = clean_model.predict(adv_data, device, target_nodes=[target_node], return_logit=True)
            _clean_logit = _clean_logit.squeeze()
            c_old = _clean_logit[clean_pred]
            c = np.max(np.delete(_clean_logit, clean_pred))
            original_score = c_old - c

            adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False)
            adv_model.train(adv_data, device)
            adv_pred, _, adv_post = adv_model.predict(adv_data, device, target_nodes=[target_node], return_posterior=True)

            # impacted_nodes = []
            # for e in _E_t:
            #     u, v = e
            #     u_sub, _, _, _ = k_hop_subgraph(u.item(), 2, data.edge_index)
            #     v_sub, _, _, _ = k_hop_subgraph(v.item(), 2, data.edge_index)
            #     impacted_nodes.extend(u_sub.tolist() + v_sub.tolist())
            # impacted_nodes = set(impacted_nodes)
            # impacted_clean_post, _, impacted_clean_post = clean_model.predict(data, device, target_nodes=list(impacted_nodes), return_posterior=True)
            # impacted_adv_post, _, impacted_adv_post = adv_model.predict(adv_data, device, target_nodes=list(impacted_nodes), return_posterior=True)
            # impacted_dist = []
            # for c_p, a_p in zip(impacted_clean_post, impacted_adv_post):
            #     impacted_dist.append(np.linalg.norm(c_p - a_p))


            k_label_dist = k_hop_subgraph_label_distribution(target_node, 2, adv_data.edge_index, adv_data.num_classes, adv_data.y)
            label_dist = k_hop_subgraph_label_distribution(target_node, 1, adv_data.edge_index, adv_data.num_classes, adv_data.y)

            print('#'*10, f'Adding {_E_t}', '#' * 10)
            print(' ' * 2 + f'Original Score:', original_score)
            # print(' ' * 2 + f'Clean Posterior:', clean_post.tolist())
            print(' ' * 2 + f'Adv Posterior:', adv_post.tolist())
            print(' ' * 2 + f'dist:', np.linalg.norm(adv_post - clean_post))
            print(' ' * 2 + f'Prediction:', adv_pred[0])
            print(' ' * 2, '1-hop label distribution:', label_dist)
            print(' ' * 2, 'k-hop label distribution:', k_label_dist)
            # print('-' * 50)
            # print(' ' * 2 + f'impacted nodes:', list(impacted_nodes))
            # print(' ' * 2 + f'impacted dist:', impacted_dist)
            print()
            

        # exit(0)
        # edge_index_t = to_undirected(torch.tensor(E_t).t())
        
        # adv_data = copy.deepcopy(data)
        # adv_data.add_edges(edge_index_t)
        # adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False)
        # adv_model.train(adv_data, device)
        # adv_pred, _ = adv_model.predict(adv_data, device, target_nodes=[target_node])

        # num_of_c = sum([math.comb(len(E_t), p) for p in range(1, len(E_t))])
        # num_of_flip = 0
        # statistic = defaultdict(int)
        # for length in range(1, len(E_t)):
        #     for _edges in combinations(E_t, length):
        #         sub_adv_nodes = [v for _, v in _edges]
        #         sub_adv_data = copy.deepcopy(adv_data)
        #         sub_adv_data.remove_edges(_edges)
        #         sub_adv_model = GNN(args, sub_adv_data.num_features, sub_adv_data.num_classes, surrogate=False)
        #         sub_adv_model.train(sub_adv_data, device)
        #         sub_adv_pred, _,  sub_adv_post = sub_adv_model.predict(sub_adv_data, device, target_nodes=[target_node], return_posterior=True)
        #         if adv_pred[0] != sub_adv_pred[0]:
        #             num_of_flip += 1
        #             statistic[len(_edges)] += 1

        # result['target node'].append(target_node)
        # result['adv edges'].append(E_t)
        # result['# of adv edges'].append(len(E_t))
        # result['# of combinations'].append(num_of_c)
        # result['# of flip'].append(num_of_flip)
        # result['surrogate prediction'].append(pred[0])
        # result['adv prediction'].append(adv_pred[0])
        # result['clean prediction'].append(clean_pred[0])
        # result['ground truth'].append(label[0])
        # result['detail'].append(dict(statistic))
    
    # df = pd.DataFrame(result)
    # _theta =  args.theta if args.theta > 0 else ''
    # result_path = os.path.join(
    #     'result', 
    #     f'setting1_q_{args.dataset}_{args.node_sampler}{_theta}_{args.num_target_nodes}_{args.edge_sampler}'
    # )

    # if args.epsilon > 0:
    #     result_path += f'_epsilon{args.epsilon}'

    # timestamp = int(time.time())
    # result_path += f'_{timestamp}.csv'
    # df.to_csv(result_path)
    # print('Finished, save the result to', result_path)
    # return df


if __name__ == '__main__':
    parser = argument.load_parser()
    parser.add_argument('--num-node-tokens', dest='num_target_nodes', type=int, default=10)
    # parser.add_argument('--num-target-nodes', dest='num_target_nodes', type=int, default=10)
    parser.add_argument('--node-sampler', dest='node_sampler', type=str, default='random', 
                        help='How to sample target nodes, distance|boundary')
    parser.add_argument('--theta', type=float, default=0.)
    parser.add_argument('--epsilon', type=float, default=-1)
    parser.add_argument('--edge-sampler', dest='edge_sampler', type=str, default='nettack')
    args = parser.parse_args()

    df_q = estimate_q(args)
    asr = utils.calc_asr(df_q)
    q = utils.calc_q(df_q)
    print('#' * 25, 'RESULT', '#' * 25)
    print('q:', q)
    print('ASR when estimate q:', asr)
    print('=' * 55)