'''
Experiment of checking the necessity of adversarial edges generated by Nettack.

Kun Wu
Stevens Institute of Technology

Command to run:
    nohup python -u experiment_setting1.py --task q 
                                           --num-node-tokens 100 
                                           --num-trials 5 -g 0 
                                           --node-sampler random 
                                           --dataset cora > setting1_q_cora_random100.out 2>&1 &

'''
import os
import time
import math
import copy
import json
import random
import pickle
import logging
from itertools import combinations
from ast import literal_eval
from collections import defaultdict
from multiprocessing import Pool
from functools import partial
from tqdm import tqdm
import numpy as np
import scipy.sparse as sp
import pandas as pd
import torch
import torch.multiprocessing as mp
from torch_geometric.utils import to_undirected, sort_edge_index, k_hop_subgraph, is_undirected
from deeprobust.graph.defense import GCN
from deeprobust.graph.global_attack import PGDAttack, MinMax
from deeprobust.graph.targeted_attack import IGAttack

import argument
import data_loader
from model.gnn import GNN
from nettack_adapter import adapte as ntk
from gf_attack_adapter import adapte as gfa
from robust_gcn_structure.certify import is_1perturbation_fragile_node
from gta import generate_topo_input, generate_masks, backdoor_attack
from gua import iterative_minimum_perturbation, minimum_attack, _iterative_minimum_perturbation
from mibtack.attack import MiBTack
from verify_attack import Minimum_MinMax_Attack
from verify_attack_new import OneShotAttack
from certified_gnn import find_non_robust_nodes
from certified_fragile_nodes import OnePerturbationFragility, CertifiedFragileness
import utils



def _result_filename(args, name):
    timestamp = time.time()
    # result_path = f'{name}_{args.target}_{args.dataset}_{args.node_sampler}{args.num_target_nodes}_{args.edge_sampler}'
    result_path = f'{name}_cvx_{args.dataset}_{args.candidate_method}{args.candidate_size}_{args.candidate_hop}hop_{args.node_sampler}{args.num_target_nodes}'
    result_path += f'_{int(timestamp)}.csv'
    return result_path

def _baseline_result_filename(args, name):
    timestamp = time.time()
    result_path = f'{name}_baseline_{args.dataset}_{args.node_sampler}{args.num_target_nodes}_{args.edge_sampler}{args.num_perts}'
    result_path += f'_{int(timestamp)}.csv'
    return result_path

def _target_model_filename(args, name):
    # timestamp = time.time()
    result_path = f'{name}_target_model_{args.dataset}_{args.node_sampler}{args.num_target_nodes}.pt'
    # result_path += f'_{int(timestamp)}.pt'
    return result_path

def _surrogate_model_filename(args, name):
    # timestamp = time.time()
    result_path = f'{name}_surrogate_model_{args.dataset}_{args.node_sampler}{args.num_target_nodes}.pt'
    # result_path += f'_{int(timestamp)}.pt'
    return result_path

def _data_filename(args, name):
    # timestamp = time.time()
    result_path = f'{name}_data_{args.dataset}_{args.node_sampler}{args.num_target_nodes}.pkl'
    # result_path += f'_{int(timestamp)}.pkl'
    return result_path

def _perturbations_filename(args, name, method):
    result_path = f'{name}_perturbations_{method}_{args.dataset}_{args.node_sampler}{args.num_target_nodes}.pkl'
    return result_path

def _escape_probability(K, K_t, q):
    return math.comb(K_t, K) * (q ** K) * ((1 - q) ** (K_t - K))

def _call_certify(pair, surrogate, data):
     v, y_v = pair
     return is_1perturbation_fragile_node(surrogate, data, v, y_v)

def find_non_robust_nodes_margin(surrogate, data, candidates, predictions, device):
    """ Find the nodes that are not robust to perturbation of size 1
    """
    pool = Pool(processes=10)
    res = list(tqdm(
            pool.imap_unordered(
            partial(_call_certify, surrogate=surrogate, data=data), 
            zip(candidates, predictions)
        ), total=len(candidates), desc='certifing'))
    # one_pert_fragile_nodes = defaultdict(list)
    non_robust_nodes = []
    for v, r in zip(candidates, res):
        if r:
            non_robust_nodes.append(v)
            # one_pert_fragile_nodes[v].append(r)
        # for c, certified in r.items():
        #     if certified:
        #         one_pert_fragile_nodes[v].append(c)
    return non_robust_nodes

    # with tqdm(total=len(candidates), desc='certifing') as pbar:
    #     for v, y_v in zip(candidates, predictions):
    #         res = is_1perturbation_fragile_node(surrogate, data, v, y_v)
    #         for c, certified in res.items():
    #             if certified:
    #                 one_pert_fragile_nodes[v].append(c)
    #         # if len(one_pert_fragile_nodes) >= 2:
    #         #     break

    #         pbar.update(1)
            
    # return one_pert_fragile_nodes


def sample_node_tokens(args, data, candidate_nodes, 
                       surrogate_model=None, posterior=None,
                       amplify=3, max_degree=math.inf):
    num_target_nodes = args.num_target_nodes * amplify

    if args.node_sampler == 'random':
        # result = random.sample(candidate_nodes, min(args.num_target_nodes, len(candidate_nodes)))
        result = []
        while len(result) < num_target_nodes:
            node = random.choice(candidate_nodes)
            if node in result:
                continue
            degree = data.degree(node)
            if degree < max_degree:
                result.append(node)
            # result.append(node)
    elif args.node_sampler == 'purity':
        _candidates = []
        for v in candidate_nodes:
            subgraph = k_hop_subgraph(v, 1, data.edge_index)[0]
            if torch.unique(data.y[subgraph]).size(0) == 1:
                _candidates.append(v)

        result = []
        while len(result) < num_target_nodes:
            v = random.choice(_candidates)
            if v in result:
                continue
            degree = data.degree(v)
            if  degree < max_degree:
                result.append(v)
            
    elif args.node_sampler == 'homophily':
        node2homophily = {}
        for v in candidate_nodes:
            if data.degree(v) == 0:
                continue

            subgraph, _edge_index, _, _ = k_hop_subgraph(v, 2, data.edge_index, relabel_nodes=True)
            node2homophily[v] = utils.homophily(
                subgraph.tolist(), 
                utils.to_directed(_edge_index).t().tolist(),
                data.y[subgraph].tolist()
            )
            # node2homophily[v] = utils.homophily_entropy(data.num_classes, data.y[subgraph])

        sorted_node2homophily = {k: v for k,v in sorted(node2homophily.items(), key=lambda item: item[1])}
        # sorted_node2homophily = {k: v for k,v in sorted(node2homophily.items(), key=lambda item: item[1], reverse=True)}
        _candidate_tokens = list(sorted_node2homophily.keys())
        if args.num_target_nodes == 1:
            v = random.choice(_candidate_tokens[:10])
            result = [v]
        else:
            i = 0
            result = []
            while len(result) < num_target_nodes:
                n = _candidate_tokens[i]
                degree = data.degree(n)
                if degree < max_degree:
                    result.append(n)
                # result.append(n)
                i += 1
            # result = _candidate_tokens[:args.num_target_nodes]
    # elif args.node_sampler == 'certify':
    #     result = find_certified_nodes(args, surrogate_model, data, utils.get_device(args))
    elif args.node_sampler == 'boundary' or args.node_sampler == 'distance':
        assert surrogate_model is not None or posterior is not None, 'Invalid input, at least one of surrogate model and posterior is not None.'
        if posterior is None:
            _, posterior = surrogate_model.predict(data, utils.get_device(args), target_nodes=candidate_nodes, return_posterior=True)

        near_boundary_nodes = {}
        for idx, p in enumerate(posterior):
            # if utils.near_boundary(z, args.k):
            near_boundary_nodes[candidate_nodes[idx]] = utils.boundary_score(p)
        sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1], reverse=args.node_sampler=='distance')}
        theta_idx = np.where(np.array(list(sorted_boundary_nodes.values())) >= args.theta)[0][0] 
        print('theta_idx', theta_idx)
        _candidate_tokens = list(sorted_boundary_nodes.keys())[theta_idx:]

        if args.num_target_nodes == 1:
            while True:
                v = random.choice(_candidate_tokens[:10])
                degree = data.degree(v)
                if degree < 10:
                    break
            # v = random.choice(_candidate_tokens[:10])
            result = [v]
        else:
            i = 0
            result = []
            while len(result) < num_target_nodes:
                try:
                    n = _candidate_tokens[i]
                except IndexError as e:
                    print('IndexError', e, ', _candidate_tokens:', _candidate_tokens, ', i=', i)
                    raise e
                degree = data.degree(n)
                if degree >= max_degree:
                    continue
                # result.append(n)
                if len(result) == 0:
                    result.append(n)
                in_2hop = False
                for c in result:
                    if data.distance(n, c) <= 2:
                        in_2hop = True
                        break
                if not in_2hop:
                    result.append(n)
                i += 1
        # result = list(sorted_boundary_nodes.keys())[:min(len(candidate_nodes), args.num_target_nodes)]
        # return {v: sorted_boundary_nodes[v] for v in result}
    elif args.node_sampler == 'cluster':
        result = [400, 1796, 2768,  166, 1735, 3170, 1638, 1870, 1919, 3211, 1113, 2318,
                  1221, 1691,  833, 2337,  159, 2948,  462, 1729, 1993, 1270,  479, 1926,
                  354, 1688, 1595, 1265,  817, 2616,  368, 2721, 1231,  816, 2671,  182,
                  2138,  674, 2658,   44, 1656,  508, 3248, 1968, 3265, 2262, 2838,  569,
                  2496, 1107, 2094, 3012,   62,   68,  412, 3257, 1863, 2043, 3086, 2836,
                  1797, 1047,   21, 1073,  623, 3182, 1817, 2562, 1390,  987,  782,  285,
                  419, 1963,  875, 1574, 2979, 1477, 2567, 2548, 2051,  605, 2296, 3084,
                  2160, 2031,   80,  678, 1446,  494, 1530, 2344, 1164, 1849, 2634, 2231,
                  1784, 3216, 2963,  319]
    elif args.node_sampler == 'first_order':
        p = torch.from_numpy(posterior)
        return torch.topk(-p.max(-1).values, k=args.num_target_nodes).indices.tolist()
    elif args.node_sampler == 'certify':
        result = find_non_robust_nodes(args, surrogate_model, data, candidate_nodes)
    elif args.node_sampler == 'margin':
        pass
        # device = utils.get_device(args)
        # assert surrogate_model is not None or posterior is not None, 'Invalid input, at least one of surrogate model and posterior is not None.'
        # if posterior is None:
        #     predictions, _ = surrogate_model.predict(data, device, target_nodes=candidate_nodes, return_posterior=True)
        # else:
        #     predictions = posterior.argmax(-1)

        # one_pert_fragile_nodes = find_1perturbation_fragile_nodes(surrogate_model.model, data, candidate_nodes, predictions, device)
        # print('one_pert_fragile_nodes', one_pert_fragile_nodes)
        # return one_pert_fragile_nodes
    else:
        raise NotImplementedError('Unknow node sampler:', args.node_sampler)

    print(f'Found {args.num_target_nodes} {args.node_sampler} target nodes nodes.')
    return result

def sample_edge_tokens_batch(args, data, target_nodes, surroget_model, predictions):
    device = utils.get_device(args)
    E_t = minimum_attack(surroget_model, data, target_nodes, predictions, device)
    return E_t 

def sample_edge_tokens(args, data, target_node, surrogate_model=None, prediction=None, label=None, target_label=None):
    if args.edge_sampler == 'nettack':
        assert surrogate_model is not None, f'Invalid input, did not find surrogate model.'
        assert prediction is not None, f'Invalid input, did not find prediction.'
        # assert label is not None, f'Invalid input, did not find label.'
        # n_perturbations = random.randint(data.degree(target_node), data.degree(target_node) * 2)
        n_perturbations = args.num_perts
        # if args.node_sampler == 'certify':
        #     n_perturbations = args.local_changes
        # elif data.degree(target_node) <= 1:
        #     n_perturbations = 2
        # else:
        #     n_perturbations = min(data.degree(target_node), 10)
        nettack = ntk(surrogate_model, data, target_node, prediction, target_label=target_label)
        nettack.reset()
        nettack.attack_surrogate(n_perturbations, perturb_structure=True, perturb_features=False, direct=True, n_influencers=1)
        E_t = nettack.structure_perturbations
        # _edge_index = to_undirected(torch.tensor(nettack.structure_perturbations).t())
    elif args.edge_sampler == 'gua':
        device = utils.get_device(args)
        # n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
        n_perturbations = 1
        # if data.degree(target_node) <= 1:
        #     n_perturbations = 2
        # else:
        #     n_perturbations = min(data.degree(target_node), 10)
        # E_t = _iterative_minimum_perturbation(surrogate_model, data, target_node, prediction[0], device, n_perturbation=n_perturbations)
        E_t = iterative_minimum_perturbation(surrogate_model, data, target_node, prediction[0], device, n_perturbation=n_perturbations)
    elif args.edge_sampler == 'gf':
        device = utils.get_device(args)
        # n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
        n_perturbations = args.num_perts
        # if data.degree(target_node) <= 1:
        #     n_perturbations = 2
        # else:
        #     n_perturbations = min(data.degree(target_node), 10)
        gf_attack = gfa(data, target_node, n_perturbations, device)
        E_t = gf_attack.structure_perturbations
    elif args.edge_sampler == 'mibtack':
        device = utils.get_device(args)
        # E_t = mibtack(args, surrogate_model, data, target_node, prediction, device)
        mibtack = MiBTack(
            surrogate_model.model, data, device, 
            target=args.target, dataset=args.dataset, verbose=False,
            explore=False
        )
        E_t = mibtack.attack(target_node, label, prediction)
    elif args.edge_sampler == 'ig':
        device = utils.get_device(args)
        surrogate = GCN(nfeat=data.num_features, nclass=data.num_classes, nhid=16, with_relu=True, with_bias=False, device=device).to(device)
        # contruct adjacency matrix
        row = data.edge_index.numpy()[0]
        col = data.edge_index.numpy()[1]
        value = np.ones((len(row)))
        adj = sp.csr_matrix((value, (row, col)), shape=(data.num_nodes, data.num_nodes))
        surrogate.fit(data.x, adj, data.y, data.train_set.nodes.tolist(), data.valid_set.nodes.tolist())
        # contruct features sparse
        row, col = np.where(data.x.numpy() == 1)
        value = np.ones((len(row)))
        x = sp.csr_matrix((value, (row, col)), shape=data.x.shape)
        labels = data.y.numpy()

        attacker = IGAttack(surrogate, data.num_nodes, attack_structure=True, attack_features=False, device=device).to(device)
        # n_perturbations = data.degree(target_node) if data.degree(target_node) > 1 else 2
        n_perturbations = args.num_perts
        # if data.degree(target_node) <= 1:
        #     n_perturbations = 2
        # else:
        #     n_perturbations = min(data.degree(target_node), 10)
        attacker.attack(x, adj, labels, data.train_set.nodes.tolist(), target_node, n_perturbations=n_perturbations, steps=10)
        # print()
        # print('# of add edges', np.sum(attacker.modified_adj - adj == 1))
        # print('# of removed edges', np.sum(attacker.modified_adj - adj == -1))
        perturbed_edges = np.array(np.where(attacker.modified_adj != adj.toarray()))
        perturbed_edges = perturbed_edges[:, perturbed_edges[0] < perturbed_edges[1]]
        # print(perturbed_edges)
        E_t = list(map(tuple, perturbed_edges.T.tolist()))
    elif args.edge_sampler == 'pgd':
        device = utils.get_device(args)
        surrogate = GCN(nfeat=data.num_features, nclass=data.num_classes, nhid=16, with_relu=True, with_bias=False, device=device).to(device)
        # contruct adjacency matrix
        row = data.edge_index.numpy()[0]
        col = data.edge_index.numpy()[1]
        value = np.ones((len(row)))
        adj = torch.sparse_coo_tensor(
            (row, col), value, size=(data.num_nodes, data.num_nodes), dtype=torch.int
        ).to_dense().numpy()
        surrogate.fit(data.x, adj, data.y, data.train_set.nodes.tolist(), data.valid_set.nodes.tolist())
        # contruct features sparse
        # row, col = np.where(data.x.numpy() == 1)
        # value = np.ones((len(row)))
        # x = sp.csr_matrix((value, (row, col)), shape=data.x.shape)
        # labels = data.y.numpy()

        attacker = PGDAttack(surrogate, data.num_nodes, loss_type='CE', device=device).to(device)
        # n_perturbations = data.degree(target_node) if data.degree(target_node) > 1 else 2
        n_perturbations = args.num_perts
        attacker.attack(data.x.numpy(), adj, data.y.numpy(), data.train_set.nodes.tolist(), target_node, n_perturbations=n_perturbations)
        modified_adj = attacker.modified_adj.cpu().numpy()
        perturbed_edges = np.array(np.where(modified_adj != adj))
        perturbed_edges = perturbed_edges[:, perturbed_edges[0] < perturbed_edges[1]]
        # print(perturbed_edges)
        E_t = list(map(tuple, perturbed_edges.T.tolist()))

    elif args.edge_sampler == 'minmax':
        device = utils.get_device(args)
        surrogate = GCN(nfeat=data.num_features, nclass=data.num_classes, nhid=16, with_relu=True, with_bias=False, device=device).to(device)
        adj = data.adjacency_matrix().to_dense().numpy()
        surrogate.fit(data.x, adj, data.y, data.train_set.nodes.tolist(), data.valid_set.nodes.tolist())

        attacker = MinMax(surrogate, data.num_nodes, loss_type='CW', device=device).to(device)
        # n_perturbations = data.degree(target_node) if data.degree(target_node) > 1 else 2
        n_perturbations = args.num_perts
        # if data.degree(target_node) <= 1:
        #     n_perturbations = 2
        # else:
        #     n_perturbations = min(data.degree(target_node), 10)
        idx_nodes = data.train_set.nodes.to(device)
        attacker.attack(data.x.numpy(), adj, data.y.numpy(), idx_nodes, target_node, n_perturbations=n_perturbations)
        modified_adj = attacker.modified_adj.cpu().numpy()
        perturbed_edges = np.array(np.where(modified_adj != adj))
        perturbed_edges = perturbed_edges[:, perturbed_edges[0] < perturbed_edges[1]]
        E_t = list(map(tuple, perturbed_edges.T.tolist()))

    elif args.edge_sampler == 'independent':
        # n_perturbations = int(data.degree(target_node)) if data.degree(target_node) > 1 else 2
        n_perturbations = random.randint(data.degree(target_node), data.degree(target_node) * 2)
        # find uninfected nodes and randomly generate edges
        k_hop_nodes, _, _, _ = k_hop_subgraph(target_node, 2, edge_index=data.edge_index)
        nodes = torch.arange(data.num_nodes)
        uninfected_nodes = nodes[~torch.isin(nodes, k_hop_nodes)]
        uninfected_edges, adv_nodes = [], []
        while len(uninfected_edges) < n_perturbations:
            u, v = random.sample(uninfected_nodes.tolist(), 2)
            comp = torch.isin(data.edge_index, torch.tensor([u, v]))
            if torch.sum(comp[0] & comp[1]) == 0:
                uninfected_edges.append([u, v])
                adv_nodes.extend([u, v]) 
        E_t = uninfected_edges
        # _edge_index = to_undirected(torch.tensor(uninfected_edges).t())
    elif args.edge_sampler == 'ours':
        attacker = OneShotAttack(surrogate_model, data)
        adj = data.adjacency_matrix()
        E_t = attacker.attack(data.x, adj, torch.tensor([prediction]), target_node)

        # attacker = Minimum_MinMax_Attack(surrogate_model, data)
        # adj = data.adjacency_matrix()
        # E_t = attacker.attack(data.x, adj, torch.tensor([prediction]), target_node)
    return E_t


def check_incompleteness(args, target_node, clean_pred, adv_pred, adv_data, triggers, device):
    # print('perturbations', triggers)
    num_of_flip = 0
    flip_back_clean = False
    failed_comb = None
    for length in range(len(triggers) - 1, 0, -1):
        for _edges in combinations(triggers, length):
            sub_adv_data = copy.deepcopy(adv_data)
            sub_adv_data.remove_edges(_edges)
            sub_adv_model = GNN(args, sub_adv_data.num_features, sub_adv_data.num_classes, surrogate=False, fix_weight=True)
            sub_adv_model.train(sub_adv_data, device)
            sub_adv_pred = sub_adv_model.predict(sub_adv_data, device, target_nodes=[target_node])            
            # print(' testing subset', _edges, 'the prediction is', sub_adv_pred[0])
            # print(f'  testing {_edges}, the prediction is {sub_adv_pred[0]}')
            # count how many subsets have diff prediction than all triggers
            if sub_adv_pred[0].item() != adv_pred:
                num_of_flip += 1

            # find a subset that flips the prediction back to be equal to clean prediction
            if clean_pred == sub_adv_pred[0]: 
                flip_back_clean = True
                failed_comb = _edges
                print(f'under the combination of {_edges}, the prediction is {sub_adv_pred[0]}, and the clean prediction is {clean_pred}')
                break

        if flip_back_clean:
            break

    return flip_back_clean, failed_comb


def _get_target_model(args, data, device):
    # suppose the verifier has the full access of dataset
    target_model = GNN(args, data.num_features, data.num_classes, surrogate=False, fix_weight=True)
    target_model.train(data, device)
    return target_model


def compare_node_samplers(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
    else:
        args.seed = random.randint(-1e+5, 1e+5)

    data = data_loader.load(args)

    target_model = _get_target_model(args, data, device)
    target_preds, target_posts = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist(), return_posterior=True)
    
    surrogate_data = copy.deepcopy(data)
    surrogate_data.train_set.y = target_preds
    surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=False)
    surrogate.train(surrogate_data, device)

    candidates = data.train_set.nodes.tolist()
    args.node_sampler = 'boundary'
    boundary_ours = sample_node_tokens(args, data, candidates, surrogate, torch.from_numpy(target_posts))

    args.node_sampler = 'first_order'
    boundary_first_order = sample_node_tokens(args, data, candidates, surrogate, torch.from_numpy(target_posts))

    print('boundary_ours', boundary_ours[:20])
    print('boundary_theirs', boundary_first_order[:20])

    print('Union size:', len(set(boundary_ours + boundary_first_order)))
    print('Intersection size:', len(set(boundary_ours).intersection(set(boundary_first_order))))

def _second_best_labels(posteriors):
    """ Get the second best label based on the posterior.
    """
    return np.argsort(posteriors, axis=1)[:, -2]

def _adv_predict_without_retrain(args, model, data, perturbations, target_nodes, device):
    adv_data = copy.deepcopy(data)
    adv_data.add_edges(to_undirected(torch.tensor(perturbations).t()))
    adv_pred = model.predict(adv_data, device, target_nodes=target_nodes)
    return adv_pred

def _adv_train_predict(args, data, perturbations, target_nodes, device):
    adv_data = copy.deepcopy(data)
    adv_data.add_edges(to_undirected(torch.tensor(perturbations).t()))
    adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
    adv.train(adv_data, device)
    adv_pred = adv.predict(adv_data, device, target_nodes=target_nodes)
    return adv_pred

def _unlearn_train_predict(args, data, perturbations, target_nodes, device):
    unlearn_data = copy.deepcopy(data)
    unlearn_data.remove_edges(perturbations)
    unlearn = GNN(args, unlearn_data.num_features, unlearn_data.num_classes, surrogate=False, fix_weight=True)
    unlearn.train(unlearn_data, device)
    unlearn_pred = unlearn.predict(unlearn_data, device, target_nodes=target_nodes)
    return unlearn_pred

def _verify(target_label, args, verifier, data, target_node, prediction, device):
    if args.candidate_method == 'label':
        # Method 1: random sample nodes based on their labels
        candidates = data.train_set.nodes[data.train_set.y == target_label].tolist()
    elif args.candidate_method == 'twohop':
        # Method 2: two-hop neighbors
        candidates = data.neighbors([target_node], l=2).tolist()
    elif args.candidate_method == 'random':
        candidates = random.sample(data.train_set.nodes.tolist(), args.candidate_size)
    else:
        raise NotImplementedError('Invalid candidate method:', args.candidate_method)

    if len(candidates) > args.candidate_size:
        candidates = random.sample(candidates, args.candidate_size)
    candidates.insert(0, target_node)

    t0 = time.time()
    res = verifier.certify(target_node, prediction, target_label, candidates=candidates, solver=args.solver)
    print(f'  ==> certifing {target_label} done, time:', time.time() - t0)

    res['candidates'] = candidates
    if res['fragile']:
        t0 = time.time()
        perturbations = res['perturbations']

        _single_predictions = []
        for e in perturbations[:5]: # Maximum 5 perturbation for speed up
            if isinstance(e, list) or isinstance(e, np.ndarray):
                e = tuple(e)
            single_adv_pred = _adv_train_predict(args, data, [e], [target_node], device)[0]
            _single_predictions.append(single_adv_pred.item())
        union_adv_pred = _adv_train_predict(args, data, [e], [target_node], device)[0].item()

        res['target_label'] = target_label
        res['single_predictions'] = _single_predictions
        res['union_predictions'] = union_adv_pred
        res['first_one_verified'] = _single_predictions[0] == target_label
        res['at_least_one_verified'] = target_label in _single_predictions
        res['all_verified'] = np.all(np.array(_single_predictions) == target_label).item()
        print(f'  ==> verify {target_label} done, time:', time.time() - t0)

    return res

def _verify_fragileness(iter_inputs, args, model, verifier, data, device):
    target_node, prediction, target_posterior = iter_inputs
    attack_label = _second_best_labels([target_posterior])[0].item()

    v_fragileness = {
        'target_node': target_node,
        'ground_truth': data.y[target_node].item(),
        'prediction': prediction,
        'second_best_label': attack_label,
        'posterior': target_posterior.tolist(),
        '1-hop label dist': data.label_distribution(target_node, 1).tolist(),
        '2-hop label dist': data.label_distribution(target_node, 2).tolist(),
        'certified': {}
    }

    # for target_label in range(data.num_classes):
    success_count = 0
    for i in range(args.num_perts):
        # if target_label == prediction:
        #     continue
        # if target_label != attack_label:
        #     continue

        if args.candidate_method == 'label':
            # Method 1: random sample nodes based on their labels
            candidates = data.train_set.nodes[data.train_set.y == attack_label].tolist()
            candidates = list(set(candidates) - set(data.adj_list[target_node]))
            
            # filter out the candidates whose majority neighbors are target label.
            # candidates = []
            # for candidate in _candidates:
            #     dist = data.label_distribution(candidate, 1)
            #     _dist = dist / np.sum(dist)
            #     if _dist[target_label] >= 0.5:
            #         candidates.append(candidate) 
        elif args.candidate_method == 'twohop':
            # Method 2: two-hop neighbors
            candidates = data.neighbors([target_node], l=2).tolist()
        elif args.candidate_method == 'random':
            candidates = random.sample(data.train_set.nodes.tolist(), args.candidate_size)
        else:
            raise NotImplementedError('Invalid candidate method:', args.candidate_method)
        
        if len(candidates) > args.candidate_size:
            candidates = random.sample(candidates, args.candidate_size)
        # candidates.insert(0, target_node)

        if args.verbose:
            print('  ==> certifing', target_node, '...')
        t0 = time.time()
        res = verifier.certify(target_node, prediction, attack_label, candidates=candidates, solver=args.solver)
        if attack_label not in v_fragileness['certified']:
            v_fragileness['certified'][attack_label] = {}

        if args.verbose:
            print('  ==> certifing', target_node, f', done, in {(time.time() - t0):.2f}s with {len(candidates)} candidates.')
        if res['error']:
            print('  ==> certifing', target_node, 'error:', res['error'])
            v_fragileness['certified'][attack_label][i] = {
                'candidates': candidates,
                'error': True,
            }
            continue

        v_fragileness['certified'][attack_label][i] = {
            'candidates': candidates,
            'neighbors': res['neighbors'],
            'perturbations': res['perturbations'],
            'adj_diff': str(res['adj_diff']),
            # 'adj': verifier._adj,
            # 'adj_pert': res['adj_pert'],
            'fragile': res['fragile'],
            'fragile_score': res['fragile_score'],
            'logit_diff_before': str(res['logit_diff_before']), 
        }

        _single_predictions = []
        _single_predictions_wo_retrain = []
        if res['fragile']:
            for idx, e in enumerate(res['perturbations'][:1]): # Maximum 5 perturbation for speed up
                if isinstance(e, list) or isinstance(e, np.ndarray):
                    e = tuple(e)
                single_adv_pred = _adv_train_predict(args, data, [e], [target_node], device)[0]
                if idx == 0:
                    if single_adv_pred != attack_label:
                        print(f'target {target_node}, pred: {prediction}, attack label {attack_label}')
                        print(f'  ==> single adv pred {single_adv_pred}, perturbation {e}')
                        for n in e:
                            data.detail_info(n)
                        print('  ==> adj diff', res['adj_diff'])
                        e2 = res['perturbations'][1]
                        double_adv_pred = _adv_train_predict(args, data, [e, e2], [target_node], device)[0]
                        if double_adv_pred == attack_label:
                            print(f'  ==> double perturbations work')
                        else:
                            print(f'  ==> double perturbations do not work, adv pred {double_adv_pred}, ')
                    else:
                        success_count += 1

                single_adv_pred_wo_retrain = _adv_predict_without_retrain(args, model, data, [e], [target_node], device)[0]
                _single_predictions_wo_retrain.append(single_adv_pred_wo_retrain.item())
                _single_predictions.append(single_adv_pred.item())
            union_adv_pred = _adv_train_predict(args, data, [e], [target_node], device)[0].item()

            v_fragileness['certified'][attack_label][i].update({
                'single_predictions':  _single_predictions,
                'single_predictions_wo_retrain': _single_predictions_wo_retrain,
                'union_predictions': union_adv_pred,
                'first_one_verified': _single_predictions[0] == attack_label,
                'at_least_one_verified': attack_label in _single_predictions,
                'all_verified': np.all(np.array(_single_predictions) == attack_label).item(),
            })
        else:
            print(target_node, 'is not fragile.')
        
        # v_fragileness['certified'][target_label] = certified
    print('The success count of 1-perturbation:', success_count)
    return v_fragileness

def _verify_fragileness_mp(args, model, data, target_node, prediction, second_label, device):
    # target_labels = list(range(data.num_classes))
    # target_labels.remove(prediction)
    target_labels = [second_label]

    verifier = CertifiedFragileness(args, model, data, device, verbose=False)
    pool = Pool(processes=len(target_labels))
    res = list(pool.imap_unordered(
            partial(
                _verify, args=args, verifier=verifier, data=data, target_node=target_node, 
                prediction=prediction, device=device
            ),
            target_labels
    ))
    certified = {}
    for r in res:
        if r['fragile']:
            certified[r['target_label']] = {
                'candidates': r['candidates'],
                'neighbors': r['neighbors'],
                'perturbations': r['perturbations'],
                'adj_diff': str(r['adj_diff']),
                'fragile': True,
                'fragile_score': r['fragile_score'],
                'logit_diff_before': str(r['logit_diff_before']),
                'single_predictions': r['single_predictions'],
                'union_predictions': r['union_predictions'],
                'first_one_verified': r['first_one_verified'],
                'at_least_one_verified': r['at_least_one_verified'],
                'all_verified': r['all_verified'],
                # 'adj_pert': r['adj_pert'],
            }
    return certified

def verify_1perturbation_fragile_nodes(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        if_fix_seed = True
    else:
        is_fix_seed = False
    
    result = defaultdict(list)
    for t in range(args.num_trials):
        if not is_fix_seed:
            args.seed = random.randint(-1e+5, 1e+5)
        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device)
        target_train_preds = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist())
        target_test_preds = target_model.predict(data, device, target_nodes=data.test_set.nodes.tolist())
        res_target = target_model.evaluate(data, device)
        print('The results of target model:', res_target)

        # surrogate_data = copy.deepcopy(data)
        # surrogate_data.train_set.y = target_train_preds 
        # surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, bias=args.edge_sampler != 'nettack', fix_weight=not args.cpf_random)
        # surrogate.train(surrogate_data, device)

        if args.node_sampler == 'margin':
            candidates = find_non_robust_nodes_margin(target_model.model, data, data.test_set.nodes.tolist(), target_test_preds, device)
        elif args.node_sampler == 'smooth':
            candidates = find_non_robust_nodes(args, target_model, data, data.test_set.nodes.tolist())
        elif args.node_sampler == 'boundary':
            candidates = sample_node_tokens(args, data, data.test_set.nodes.tolist(), target_model, amplify=1)
        elif args.node_sampler == 'high_degree':
            node2degree = {v: data.degree(v) for v in data.test_set.nodes.tolist()}
            sorted_node2degree = {k: v for k,v in sorted(node2degree.items(), key=lambda item: item[1], reverse=True)}
            candidates = list(sorted_node2degree.keys())[:args.num_target_nodes]
        elif args.node_sampler == 'all':
            if args.num_target_nodes == -1:
                candidates = data.test_set.nodes.tolist()
            else:
                candidates = random.sample(data.test_set.nodes.tolist(), args.num_target_nodes)
        else:
            raise ValueError('Invalid node sampler:', args.node_sampler)
        
        print(f'{args.node_sampler} sampler found {len(candidates)} candidates.')

        target_preds, posteriors = target_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        second_best_labels = _second_best_labels(posteriors)
        verifier = CertifiedFragileness(args, target_model, data, device, verbose=False)

        for i in range(len(candidates)):
            _verify_fragileness((candidates[i], target_preds.tolist()[i], posteriors[i]), args, target_model, verifier, data, device)
        exit(0)

        certified_fragile_num = 0
        fargile_but_failed_num = 0
        pool = Pool(processes=5)
        nodes_fragileness_result = list(tqdm(
            pool.imap_unordered(
                partial(_verify_fragileness, args=args, model=target_model, verifier=verifier, data=data, device=device),
                zip(candidates, target_preds.tolist(), posteriors)
            ),
            total=len(candidates), desc=f'At trial {t}, certifing'
        ))
        pool.close()

        pert_results = []
        for res in nodes_fragileness_result:
            """ {
                'target_node': xx,
                'prediction': xx,
                'second_best_label: xx,
                'bundary_score': xx,
                'certified': {
                    'x': {
                        'target_label': x,
                        'perturbations': [(u1, v1), (u2, v2), ...]
                        'adj_diff.': [xx, xx, ...]
                        'fragile': True, 
                        'fragile_score': 0.1,
                        'single_predictions': [],
                        'union_predictions': []
                    }, ...
                }
            }
            """

            failed_count = 0
            for i in range(args.num_perts):
                if 'first_one_verified' in res['certified'][res['second_best_label']][i]:
                    if res['certified'][res['second_best_label']][i]['fragile']:
                        certified_fragile_num += 1
                    if res['certified'][res['second_best_label']][i]['fragile'] and not res['certified'][res['second_best_label']][i]['first_one_verified']:
                        # print('!!!!!!!!!!', res['target_node'], 'failed to verify', res['second_best_label'])
                        failed_count += 1 
                    # pert_results.append(res['certified'][res['second_best_label']][i]['first_one_verified']) 
                    # if res['certified'][res['second_best_label']][i]['fragile']:
                    #     certified_fragile_num += 1
            if failed_count > 0:
                print('Failed:', res['target_node'], 'failed to verify', res['second_best_label'], 'for', failed_count, 'times')
                print()
                fargile_but_failed_num += 1

            # if 'first_one_verified' in res['certified'][res['second_best_label']]:
            #     if not res['certified'][res['second_best_label']]['first_one_verified']:
            #         print('!!!!!!!!!!', res['target_node'], 'failed to verify', res['second_best_label'])

            #     pert_results.append(res['certified'][res['second_best_label']]['first_one_verified']) 
            #     if res['certified'][res['second_best_label']]['fragile']:
            #         certified_fragile_num += 1
                # if not res['certified'][res['second_best_label']]['first_one_verified']:
                #     print(res)
                #     print()
                #     exit(0)

            # if res['second_best_label'] in res['certified']:
            #     del res['certified'][res['second_best_label']]['adj']
            #     del res['certified'][res['second_best_label']]['adj_pert']
            result[t].append(res)

        # print('  ==> the number of certified fragile nodes:', certified_fragile_num)
        print('Trial', t)
        print('  ==> the number of certified fragile nodes:', certified_fragile_num)
        print('  ==> Among those nodes, the number of nodes that 1-pert failed:', fargile_but_failed_num)
        print()

    result_file = _result_filename(args, 'fragile')
    try:
        with open(result_file, 'w') as fp:
            json.dump(dict(result), fp, indent=4)
    except TypeError as err:
        print('result', result)
        raise err

    print('-' * 80)
    print('  ', 'Experiment results are saved to', result_file)
    print('-' * 80)

        # iter = tqdm(one_pert_fragile_nodes.items() if isinstance(one_pert_fragile_nodes, dict) else one_pert_fragile_nodes, desc='verify')
        # for item in iter:
        #     # if isinstance(item, tuple):
        #     #     v, fragile_classes = item
        #     #     fragile_class = fragile_classes[0]
        #     # else:
        #     #     v = item
        #     #     fragile_class = None

        #     v, label2perturbations = item
        #     clean_pred = target_model.predict(data, device, target_nodes=[v])
        #     print('Attacking node', v, ', prediction:', clean_pred[0])
        #     for target_label, perturbations in label2perturbations.items():
        #         perturbation2pred = {}
        #         verification_edges = []
        #         for e in random.sample(perturbations, 10) if len(perturbations) > 10 else perturbations:
        #             if isinstance(e, list) or isinstance(e, np.ndarray):
        #                 e = tuple(e)

        #             adv_data = copy.deepcopy(data)
        #             adv_data.add_edges(torch.tensor([[e[0], e[1]], [e[1], e[0]]]))

        #             adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
        #             adv.train(adv_data, device)

        #             adv_pred = adv.predict(adv_data, device, target_nodes=[v])
        #             perturbation2pred[e] = adv_pred[0]
        #             if adv_pred[0] == target_label:
        #                 verification_edges.append(e)
            
        #         # apply all the pontential perturbations at once
        #         adv_data = copy.deepcopy(data)
        #         adv_data.add_edges(to_undirected(torch.tensor(perturbations).t()))
        #         adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
        #         adv.train(adv_data, device)

        #         adv_pred = adv.predict(adv_data, device, target_nodes=[v])
        #         print(f'  ==> fragile on label {target_label}, potential perturbations are {perturbations}, single pert predictions: {perturbation2pred}, union prediction: {adv_pred}')
        #         print()

            # for edge_sampler in ['nettack', 'ours', 'mibtack']:
            # for edge_sampler in ['nettack']:
            #     args.edge_sampler = edge_sampler
            #     perturbations = sample_edge_tokens(args, data, v, surrogate, clean_pred[0], data.y[v], target_label=fragile_class)
            #     print('perturbations', perturbations)

            #     if perturbations is None or len(perturbations) == 0:
            #         result['trial'].append(t)
            #         result['target'].append(v)
            #         result['label'].append(data.y[v])
            #         result['clean prediction'].append(clean_pred[0])
            #         result['target class'].append(fragile_class)
            #         result['method'].append(edge_sampler)
            #         result['perturbations'].append([])
            #         result['single adv predictions'].append([])
            #         result['adv prediction'].append(-1)
            #         result['sub adv prediction'].append(-2)
            #         result['fragileness'].append(0)
            #         result['consistency'].append(-1)
            #         result['incomplete'].append(-1)
            #         continue

                # consistency check
                # perturbation2pred = {}
                # verification_edges = []
                # for e in random.sample(perturbations, 10) if len(perturbations) > 10 else perturbations:
                #     adv_data = copy.deepcopy(data)
                #     adv_data.add_edges(torch.tensor([[e[0], e[1]], [e[1], e[0]]]))

                #     adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
                #     adv.train(adv_data, device)

                #     adv_pred = adv.predict(adv_data, device, target_nodes=[v])
                #     perturbation2pred[e] = adv_pred[0]
                #     if adv_pred[0] == fragile_class:
                #         verification_edges.append(e)
                
                # result['trial'].append(t)
                # result['target'].append(v)
                # result['label'].append(data.y[v].item())
                # result['clean prediction'].append(clean_pred[0])
                # result['target class'].append(fragile_class)
                # result['method'].append(edge_sampler)
                # result['perturbations'].append(perturbations)
                # result['single adv predictions'].append(list(perturbation2pred.values()))
                # result['fragileness'].append(1 if len(verification_edges) > 0 else 0)
                # result['consistency'].append(1 if np.sum(np.array(list(perturbation2pred.values())) != fragile_class) == 0 else 0)

                # verification
                # if len(verification_edges) > 1:
                #     adv_data = copy.deepcopy(data)
                #     edge_index_t = to_undirected(torch.tensor(verification_edges).t()) 
                #     adv_data.add_edges(edge_index_t)
                #     adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
                #     adv.train(adv_data, device)
                #     adv_pred = adv.predict(adv_data, device, target_nodes=[v])
                #     result['adv prediction'].append(adv_pred[0])

                #     if check_incompleteness(args, v, clean_pred[0], adv_pred[0], adv_data, verification_edges, device):
                #         # print('Yes, it is incompleteness!')
                #         result['sub adv prediction'].append(clean_pred[0])
                #         result['incomplete'].append(1)
                #     else:
                #         # print('No, not incompleteness!')
                #         result['sub adv prediction'].append(adv_pred[0])
                #         result['incomplete'].append(0)
                # else:
                #     result['adv prediction'].append(-1)
                #     result['sub adv prediction'].append(-2)
                #     result['incomplete'].append(-1)

                # if len(verification_edges) > 1:
                #     adv_data = copy.deepcopy(data)
                #     edge_index_t = to_undirected(torch.tensor(perturbations).t()) 
                #     adv_data.add_edges(edge_index_t)
                #     adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
                #     adv.train(adv_data, device)
                #     adv_pred = adv.predict(adv_data, device, target_nodes=[v])
                #     result['union adv prediction'].append(adv_pred[0])
                # else:
                #     result['union adv prediction'].append(-1)
    
    # df = pd.DataFrame(result)

    # for edge_sampler in ['nettack', 'ours', 'mibtack']:
    # for edge_sampler in ['nettack']:
    #     _df = df[df['method'] == edge_sampler]
    #     num_fraigle = 0
    #     num_inconsistent = 0
    #     for idx, row in _df.iterrows():
    #         # single_adv_predictions = literal_eval(row['single adv predictions'])
    #         single_adv_predictions = row['single adv predictions']
    #         clean_pred = row['clean prediction']
    #         target_class = row['target class']
    #         if (np.array(single_adv_predictions) != target_class).sum() > 0:
    #             num_inconsistent += 1
    #         for single_pred in single_adv_predictions:
    #             if single_pred != clean_pred:
    #                 num_fraigle += 1
    #                 break
    #     print(f'The certified fragile nodes are {num_fraigle} out of {len(_df)} nodes.')
    #     print(f'The inconsistent nodes are {num_inconsistent} out of {len(_df)} nodes.')

    #     args.edge_sampler = edge_sampler
    #     result_filename = _result_filename(args, 'fragile')
    #     _df.to_csv(os.path.join('result', result_filename))
    #     print('Verifing the fragility is Done, the result file is saved to', result_filename)


def _verify_1perturbation_fragile(data, v, prediction, perturbations, device):
    verification_edges = []
    verification_preds = []
    for e in perturbations:
        adv_data = copy.deepcopy(data)
        adv_data.add_edges(torch.tensor([[e[0],e[1]], [e[1],e[0]]]))

        adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
        adv.train(adv_data, device)

        adv_pred = adv.predict(adv_data, device, target_nodes=[v])
        if adv_pred[0] != prediction:
            verification_edges.append(e)
            verification_preds.append(adv_pred[0])

    # print('verification_edges', verification_edges)
    # print('verification_preds', verification_preds)
    return verification_edges, verification_preds


def certify_fragile_with_perturbations(inputs, args, verifier, data):
    target_node, prediction, target_posterior = inputs
    attack_label = _second_best_labels([target_posterior])[0].item()

    num_iter = 0
    perturbations = []
    while len(perturbations) < args.num_perts:
        if args.candidate_method == 'label':
            # Method 1: random sample nodes based on their labels
            candidates = data.train_set.nodes[data.train_set.y == attack_label].tolist()
            candidates = list(set(candidates) - set(data.adj_list[target_node]))
            # filter out the candidates whose majority neighbors are target label.
            # candidates = []
            # for candidate in _candidates:
            #     dist = data.label_distribution(candidate, 1)
            #     _dist = dist / np.sum(dist)
            #     if _dist[attack_label] >= 0.5:
            #         candidates.append(candidate) 
        elif args.candidate_method == 'random':
            candidates = random.sample(data.train_set.nodes.tolist(), args.candidate_size)
        else:
            raise NotImplementedError('Invalid candidate method:', args.candidate_method)
        
        if len(candidates) > args.candidate_size:
            candidates = random.sample(candidates, args.candidate_size)
        # candidates.insert(0, target_node)

        if args.verbose:
            print('  ==> certifing', target_node, '...')
        t0 = time.time()
        res = verifier.certify(target_node, prediction, attack_label, candidates=candidates, solver=args.solver)
        if args.verbose:
            print('  ==> certifing', target_node, f', done, in {(time.time() - t0):.2f}s')
        if res['fragile']:
            p = tuple(res['perturbations'][0])
            if p not in perturbations:
                perturbations.append(p)
        if num_iter > 10:
            break
        num_iter += 1

    # for e in perturbations:
    #     if isinstance(e, list) or isinstance(e, np.ndarray):
    #         e = tuple(e)
    #     single_adv_pred = _adv_train_predict(args, data, [e], [target_node], utils.get_device(args))[0]
    #     if single_adv_pred != attack_label:
    #         print(f'target {target_node}, pred: {prediction}, attack label {attack_label}')
    #         print(f'  ==> single adv pred {single_adv_pred}, perturbation {e}')
    
    if len(perturbations) < 3:
        return None
    else:
        return perturbations
    

def verify_unlearning_backdoor(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fixed_seed = True
    else:
        is_fixed_seed = False

    result = defaultdict(list)
    for t in range(args.num_trials):
        if not is_fixed_seed:
            args.seed = random.randint(0, 1e+5)
            torch.manual_seed(args.seed)
            torch.cuda.manual_seed_all(args.seed)
            np.random.seed(args.seed)
            random.seed(args.seed)

        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device)
        target_train_preds = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist())

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.y = target_train_preds
        surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=False)
        surrogate.train(surrogate_data, device)

        if args.node_sampler == 'boundary':
            candidates = sample_node_tokens(args, data, data.test_set.nodes.tolist(), target_model, amplify=1)
        elif args.node_sampler == 'random':
            candidates = random.sample(data.test_set.nodes.tolist(), args.num_target_nodes)
        target_preds, posteriors = target_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        # attack_labels = _second_best_labels(posteriors)

        """ Add hyper-parameters of GTA, details can be found in gta.py
        """
        _args = copy.deepcopy(args)
        _args.trigger_size = 3
        _args.num_triggers = 10
        _args.bilevel_steps = 4
        _args.gtn_lr = 0.01
        _args.topo_thrd = 0.5
        _args.gtn_epochs = 10
        _args.topo_activation = 'sigmoid'
        _args.gta_batch_size = 16
        _args.lr_decay_steps = [25, 35]
        _args.train_epochs = 40
        _args.lambd = 1
        _args.gtn_input_type = '2hop'
        _args.feat_perb = False

        # Call GTA and get backdoored model, topo generator, and backdoored edges
        bkd_model, topo_net, bkd_edges, bkd_label = backdoor_attack(_args, surrogate, data)
        bkd_data = copy.deepcopy(data)
        bkd_data.add_edges(to_undirected(torch.tensor(bkd_edges).t()))
        bkd_adj = bkd_data.adjacency_matrix().to_dense().to(device)

        subset_retrain_mdoel = {}
        for v, pred in tqdm(zip(candidates, target_preds), total=len(candidates), desc='verify'):
            result['trial'].append(t)
            result['target node'].append(v)
            result['ground truth'].append(data.y[v].item())
            result['prediction'].append(pred)
            result['attack_label'].append(bkd_label)

            subset, edge_index, _, _ = k_hop_subgraph(v, 2, bkd_data.edge_index)
            # print('subset:', subset)
            subgraph = random.sample(subset.tolist(), min(len(subset), _args.trigger_size))

            # obtain trigger of the target node
            topo_input, feat_input = generate_topo_input(_args, bkd_data, device)
            topo_mask, _ = generate_masks(bkd_data, [(subgraph, None)], device)
            rst_bkd_adj = topo_net(topo_input, topo_mask, _args.topo_thrd, device, _args.topo_activation, 'topo')
            trigger_edges = torch.nonzero(rst_bkd_adj * (1 - bkd_adj)).tolist()
            if len(trigger_edges) == 0:
                result['incomplete prediction'].append(-1)
                result['incomplete'].append(0)
                result['failed perturbation'].append([])
                result['bkd prediction'].append(-1)
                continue

            _trigger_data = copy.deepcopy(bkd_data)
            _trigger_data.add_edges(to_undirected(torch.tensor(trigger_edges).t()))

            bkd_pred = bkd_model.predict(_trigger_data, device, target_nodes=[v])
            result['bkd prediction'].append(bkd_pred[0])

            equal_to_bkd_label = True
            for length in range(len(bkd_edges) - 1, 0, -1):
                for _edges in combinations(bkd_edges, length):
                    sub_bkd_data = copy.deepcopy(bkd_data)
                    sub_bkd_data.remove_edges(_edges)

                    # print('_edges:', _edges)
                    _edges_key = tuple(map(tuple, _edges))
                    if _edges_key in subset_retrain_mdoel:
                        sub_bkd_model = subset_retrain_mdoel[_edges_key]
                    else:
                        sub_bkd_model = GNN(args, sub_bkd_data.num_features, sub_bkd_data.num_classes, surrogate=False)
                        sub_bkd_model.train(sub_bkd_data, device)
                        subset_retrain_mdoel[_edges_key] = sub_bkd_model
                    
                    sub_bkd_pred = sub_bkd_model.predict(sub_bkd_data, device, target_nodes=[v])
                    if sub_bkd_pred[0] != bkd_label:
                        equal_to_bkd_label = False
                        break
                if not equal_to_bkd_label:
                    break
            if not equal_to_bkd_label:
                result['incomplete prediction'].append(sub_bkd_pred[0].item())
                result['incomplete'].append(1)
                result['failed perturbation'].append(_edges)
            else:
                result['incomplete prediction'].append(-1)
                result['incomplete'].append(0)
                result['failed perturbation'].append([])

        _df = pd.DataFrame(result)
        trial_df = _df[_df['trial'] == t]
        fpr = utils.calc_fpr(trial_df, len(trial_df))
        fnr = utils.calc_fnr(trial_df, len(trial_df))
        print(f' At trial {t}, the result of {args.edge_sampler}', '-' * 80)
        print('  => FPR:', fpr)
        print('  => FNR:', fnr)
        print('=' * 100)

    try:    
        df = pd.DataFrame(result)
    except ValueError as err:
        print('result', result)
        raise err

    for t in range(args.num_trials):
        _df = df[df['trial'] == t]
        print(f'Trial {t}, the number of incompleteness:', _df['incomplete'].values.sum())
        # print(f'Trial {t}, the number of fragile:', _df['fragileness'].values.sum())
    # incompleteness = df['incomplete'].values.sum()
    # print('  ==> The incompleteness:', incompleteness)

    result_filename = _baseline_result_filename(args, 'verify')
    df.to_csv(os.path.join('result', result_filename))
    print('-' * 80)
    print('Verifing unlearning is done, the result file is saved to', result_filename)
    print('-' * 80)
    

def verify_unlearning_baselines(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fixed_seed = True
    else:
        is_fixed_seed = False

    result = defaultdict(list)
    for t in range(args.num_trials):
        if not is_fixed_seed:
            args.seed = random.randint(0, 1e+5)
            torch.manual_seed(args.seed)
            torch.cuda.manual_seed_all(args.seed)
            np.random.seed(args.seed)
            random.seed(args.seed)

        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device)
        target_train_preds = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist())
        # target_test_preds, target_test_posts = target_model.predict(data, device, target_nodes=data.test_set.nodes.tolist(), return_posterior=True)
        
        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.y = target_train_preds
        if args.edge_sampler == 'nettack':
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True, bias=False)
        else:
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True)
        surrogate.train(surrogate_data, device)
        # surr_test_preds = surrogate.predict(surrogate_data, device, target_nodes=data.test_set.nodes.tolist())

        if args.node_sampler == 'boundary':
            candidates = sample_node_tokens(args, data, data.test_set.nodes.tolist(), target_model, amplify=1)
        elif args.node_sampler == 'random':
            candidates = random.sample(data.test_set.nodes.tolist(), args.num_target_nodes)
        target_preds, posteriors = target_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        attack_labels = _second_best_labels(posteriors)

        for v, pred, attack_label in tqdm(zip(candidates, target_preds, attack_labels), total=len(candidates), desc='verify'):
            result['trial'].append(t)
            result['target node'].append(v)
            result['ground truth'].append(data.y[v].item())
            result['prediction'].append(pred)
            result['attack_label'].append(attack_label)
            # result['fragileness'].append(False if perturbations is None else True)

            t0 = time.time()
            perturbations = sample_edge_tokens(args, data, v, surrogate, pred, target_label=attack_label)
            result['attack time'].append(time.time() - t0)

            # verification
            if perturbations is None or len(perturbations) == 0:
                result['perturbations'].append([])
                result['adv prediction'].append(-1)
                result['incomplete prediction'].append(-1)
                result['incomplete'].append(0)
                result['failed perturbation'].append([])
                # result['single adv predictions'].append([])
                continue
            result['perturbations'].append(perturbations)

            adv_data = copy.deepcopy(data)
            edge_index_t = to_undirected(torch.tensor(perturbations).t()) 
            adv_data.add_edges(edge_index_t)
            adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=False)
            adv.train(adv_data, device)
            adv_pred = adv.predict(adv_data, device, target_nodes=[v])
            result['adv prediction'].append(adv_pred[0])

            if len(perturbations)  == 1:
                result['incomplete prediction'].append(-1)
                result['incomplete'].append(0)
                result['failed perturbation'].append([])
                # result['single adv predictions'].append([])
                continue
            
            is_incomplete, _failed_pert = check_incompleteness(args, v, pred, adv_pred[0], adv_data, perturbations, device)
            if is_incomplete:
                result['incomplete prediction'].append(pred)
                result['incomplete'].append(1)
                result['failed perturbation'].append(_failed_pert)
                # double check the perturbations
                # single_adv_predictions = []
                # for e in perturbations:
                #     if isinstance(e, list) or isinstance(e, np.ndarray):
                #         e = tuple(e)
                #     single_adv_pred = _adv_train_predict(args, data, [e], [v], device)[0]
                #     single_adv_predictions.append(single_adv_pred.item())
                # result['single adv predictions'].append(single_adv_predictions)

                print('Yes, it is incompleteness!')
                print('  ==> Perturbations:', perturbations)
                print('  ==> The failed perturbations:', _failed_pert)
                # print('  ==> The single adv predictions:', single_adv_predictions)
            else:
                # print('No, not incompleteness!')
                result['incomplete prediction'].append(-1)
                result['incomplete'].append(0)
                result['failed perturbation'].append([])
                # result['single adv predictions'].append([])
        _df = pd.DataFrame(result)
        trial_df = _df[_df['trial'] == t]
        fpr = utils.calc_fpr(trial_df, len(trial_df))
        fnr = utils.calc_fnr(trial_df, len(trial_df))
        print(f' At trial {t}, the result of {args.edge_sampler}', '-' * 80)
        print('  => FPR:', fpr)
        print('  => FNR:', fnr)
        print('=' * 100)

    try:    
        df = pd.DataFrame(result)
    except ValueError as err:
        print('result', result)
        raise err

    for t in range(args.num_trials):
        _df = df[df['trial'] == t]
        print(f'Trial {t}, the number of incompleteness:', _df['incomplete'].values.sum())
        # print(f'Trial {t}, the number of fragile:', _df['fragileness'].values.sum())
    # incompleteness = df['incomplete'].values.sum()
    # print('  ==> The incompleteness:', incompleteness)

    result_filename = _baseline_result_filename(args, 'verify')
    df.to_csv(os.path.join('result', result_filename))
    print('-' * 80)
    print('Verifing unlearning is done, the result file is saved to', result_filename)
    print('-' * 80)


def verify_unlearning(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fixed_seed = True
    else:
        is_fixed_seed = False

    result = defaultdict(list)
    for t in range(args.num_trials):
        logging.info('Start verifing unlearning at trial %d ...', t)
        if not is_fixed_seed:
            args.seed = random.randint(0, 1e+5)
            torch.manual_seed(args.seed)
            torch.cuda.manual_seed_all(args.seed)
            np.random.seed(args.seed)
            random.seed(args.seed)

        data = data_loader.load(args)

        logging.info('  ==> Loading the target model ...')
        target_model = _get_target_model(args, data, device)
        target_train_preds = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist())
        # target_test_preds, target_test_posts = target_model.predict(data, device, target_nodes=data.test_set.nodes.tolist(), return_posterior=True)

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.y = target_train_preds
        # surr_test_preds = surrogate.predict(surrogate_data, device, target_nodes=data.test_set.nodes.tolist())

        logging.info('  ==> Sampling node tokens ...')
        if args.node_sampler == 'boundary':
            candidates = sample_node_tokens(args, data, data.test_set.nodes.tolist(), target_model, amplify=1)
        elif args.node_sampler == 'random':
            candidates = random.sample(data.test_set.nodes.tolist(), args.num_target_nodes)
        target_preds, posteriors = target_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        attack_labels = _second_best_labels(posteriors)

        for method in ['ours', 'nettack']:
            t0 = time.time()
            if method == 'nettack' or method == 'minmax':
                surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True, bias=False, fix_weight=False)
                surrogate.train(data, device)
                surrogate_preds = surrogate.predict(data, device, target_nodes=candidates)

                args.edge_sampler = method
                target_nodes_perts = []
                for v, pred, attack_label in tqdm(zip(candidates, target_preds, attack_labels), total=len(candidates), desc=f'At trial {t}, {method} attacking'):
                    perturbations = sample_edge_tokens(args, data, v, surrogate, pred, target_label=attack_label)
                    target_nodes_perts.append(perturbations)
            elif method == 'ours':
                logging.info('  ==> Loading the surrogate model ...') 
                surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=False, fix_weight=False)
                surrogate.train(data, device)

                logging.info('  ==> start to run CVX searching ...')
                verifier = CertifiedFragileness(args, surrogate, data, device, verbose=False)
                # for c, ta, p in zip(candidates, target_preds.tolist(), posteriors):
                #     certify_fragile_with_perturbations((c, ta, p), args, verifier, data)
                pool = Pool(processes=5)
                target_nodes_perts = list(tqdm(
                    pool.imap(
                        partial(certify_fragile_with_perturbations, args=args,verifier=verifier, data=data),
                        zip(candidates, target_preds.tolist(), posteriors)
                    ),
                    total=len(candidates), desc=f'At trial {t}, certifing'
                ))
                pool.close()
            else:
                raise NotImplementedError('Invalid method:', method)
            total_preparation_time = time.time() - t0

            """ Option 2
                Enumerate all the subset of perturbations and check the incompleteness.
            """
            # tp_count = 0
            # incomplete_count = 0
            # for candidate, perturbation, pred, attack_label in tqdm(zip(candidates, target_nodes_perts, target_preds, attack_labels), total=len(candidates), desc=f'At trial {t}, {method} verifying'):
            #     result['trial'].append(t)
            #     result['method'].append(method)
            #     result['total_preparation_time'].append(total_preparation_time)
            #     result['target node'].append(candidate)
            #     result['ground truth'].append(data.y[candidate].item())
            #     result['prediction'].append(pred)
            #     result['attack_label'].append(attack_label)

            #     if perturbation is None or len(perturbation) == 0:
            #         result['perturbations'].append([])
            #         result['adv prediction'].append(pred)
            #         result['incompleteness'].append(1)
            #         result['failed sub perturbation'].append([])
            #         result['verification time'].append(0)
            #         continue
            #     result['perturbations'].append(perturbation)

            #     adv_data = copy.deepcopy(data)
            #     edge_index_t = to_undirected(torch.tensor(perturbation).t()) 
            #     adv_data.add_edges(edge_index_t)
            #     adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
            #     adv.train(adv_data, device)
            #     adv_pred = adv.predict(adv_data, device, target_nodes=[candidate])
            #     result['adv prediction'].append(adv_pred[0])
            #     if adv_pred[0] != pred:
            #         tp_count += 1

            #     # print('candidate', candidate, 'attack label', attack_label, 'prediction', pred, 'adv prediction', adv_pred[0])
            #     t0 = time.time()
            #     is_incomplete, _failed_pert = check_incompleteness(args, candidate, pred, adv_pred[0], adv_data, perturbation, device)
            #     if is_incomplete:
            #         # result['incomplete prediction'].append(pred)
            #         result['incompleteness'].append(1)
            #         result['failed sub perturbation'].append(_failed_pert)
            #         incomplete_count += 1
            #         # result['failed perturbation'].append(_failed_pert)
            #         # # double check the perturbations
            #         # single_adv_predictions = []
            #         # for e in perturbations:
            #         #     if isinstance(e, list) or isinstance(e, np.ndarray):
            #         #         e = tuple(e)
            #         #     single_adv_pred = _adv_train_predict(args, data, [e], [v], device)[0]
            #         #     single_adv_predictions.append(single_adv_pred.item())
            #         # result['single adv predictions'].append(single_adv_predictions)
            #     else:
            #         result['incompleteness'].append(0)
            #         result['failed sub perturbation'].append([])
            #     result['verification time'].append(time.time() - t0)
            # print(f'Trial {t}, method {method}, the number of incompleteness:', incomplete_count)
            # print(f'At trial {t}, method {method}, Option 2:')
            # print('  ==> TPR:', tp_count / len(candidates))
            # print('  ==> FPR:', 1 - tp_count / len(candidates))
            # print('  ==> TNR:', 1 - incomplete_count / len(candidates))
            # print('  ==> FNR:', incomplete_count / len(candidates))
            # print('=' * 80)

            """ Option 1
                Simulate the behavior of malicious service provider
                that randomly sample p% edges as a subset and do the unlearning or retraining.
            """

            # for num_node_tokens in [10, 20, 30, 40, 50]:
            for num_node_tokens in [50]:
                _candidates, _perturbations, _predictions, _attack_label = [], [], [], []
                for candidate, perturbation, pred, attack_label in zip(candidates[:num_node_tokens], target_nodes_perts[:num_node_tokens], target_preds[:num_node_tokens], attack_labels[:num_node_tokens]):
                    if perturbation is not None and len(perturbation) > 1:
                        _candidates.append(candidate)
                        _perturbations.append(perturbation)
                        _predictions.append(pred)
                        _attack_label.append(attack_label)
                # _candidates = np.array(_candidates)
                # _perturbations = np.array(_perturbations)
                all_perturbations = [p for pert in _perturbations for p in pert]
                _predictions = np.array(_predictions)
                _attack_label = np.array(_attack_label)

                # add perturbations to the graph
                adv_predictions = _adv_train_predict(args, data, all_perturbations, _candidates, device)
                # how many nodes are successfully attacked
                consist_asr = np.sum(adv_predictions == _attack_label) / len(_candidates)
                asr = np.sum(adv_predictions != _predictions) / len(_candidates)
                adv_data = copy.deepcopy(data)
                adv_data.add_edges(to_undirected(torch.tensor(all_perturbations).t()))

                tp, tn, fp, fn = {}, {}, {}, {}
                for p in args.percentages:
                    tp[p], tn[p], fp[p], fn[p] = 0, 0, 0, 0
                    t0 = time.time()
                    for idx in tqdm(range(len(_candidates)), desc=f'At trial {t}, {method} verifying {p}%'):
                        """ Temporarily Add
                            Each trail only has one target node, and randomly sample 1, 2, 3, 4 perturbations as a subset.
                        """

                        # rand_idx = random.choice(range(len(_candidates)))
                        trial_v = _candidates[idx]
                        trial_perts = _perturbations[idx]
                        if len(trial_perts) != 5:
                            logging.warning('The number of perturbations is less than 5, but %d', len(trial_perts))
                        # trial_pred = _predictions[idx]

                        adv_pred = adv_predictions[idx]

                        neg_pred = _unlearn_train_predict(args, adv_data, trial_perts, [trial_v], device)
                        if neg_pred[0] != adv_pred:
                            tn[p] += 1
                        else:
                            fn[p] += 1

                        sub_perts = random.sample(trial_perts, max(int(len(trial_perts) * p), 1))
                        pos_pred = _unlearn_train_predict(args, adv_data, sub_perts, [trial_v], device)
                        if pos_pred[0] == adv_pred:
                            tp[p] += 1
                        else:
                            fp[p] += 1

                        # # server no cheating
                        # neg_preds = _unlearn_train_predict(args, adv_data, all_perturbations, _candidates, device)
                        # if np.sum(neg_preds == adv_predictions) == 0: # all the node tokens label changed
                        #     tn[p] += 1
                        # if np.sum(neg_preds == adv_predictions) > 0: # at least one token did not change
                        #     fp[p] += 1

                        # # server cheating
                        # sub_perts = random.sample(all_perturbations, max(int(len(all_perturbations) * p), 1))
                        # pos_preds = _unlearn_train_predict(args, adv_data, sub_perts, _candidates, device)
                        # if np.sum(pos_preds == adv_predictions) > 0: # at least one token did not change
                        #     tp[p] += 1
                        # if np.sum(pos_preds == adv_predictions) == 0: # all the node tokens label changed
                        #     fp[p] += 1
                        # _result.append(verification_prob.item())
                            

                    verify_time = time.time() - t0
                    result['trial'].append(t)
                    result['method'].append(method)
                    result['total_preparation_time'].append(total_preparation_time)
                    result['num node tokens'].append(num_node_tokens)
                    result['p'].append(p)
                    result['consist asr'].append(consist_asr)
                    result['asr'].append(asr)
                    result['tp'].append(tp[p])
                    result['tn'].append(tn[p])
                    result['fp'].append(fp[p])
                    result['fn'].append(fn[p])    
                    result['verify time'].append(verify_time)

            # print(f'At trial {t}, method {method}, Option 1:')
            # print('  ==>', json.dumps(summary, indent=4))
            # print('=' * 80)

        ts = int(time.time())
        target_model_filename = _target_model_filename(args, f'main{t}_{ts}')
        target_model.save_model(os.path.join('intermediate', target_model_filename))
        surrogate_model_filename = _surrogate_model_filename(args, f'main{t}_{ts}')
        surrogate.save_model(os.path.join('intermediate', surrogate_model_filename))
        data_filename = _data_filename(args, f'main{t}_{ts}')
        with open(os.path.join('intermediate', data_filename), 'wb') as fp:
            pickle.dump(data, fp)
        perturbations_filename = _perturbations_filename(args, f'main{t}_{ts}', method)
        with open(os.path.join('intermediate', perturbations_filename), 'wb') as fp:
            pickle.dump(target_nodes_perts, fp)

        trial_df = pd.DataFrame(result)
        trial_df = trial_df[trial_df['trial'] == t]
        trial_df.to_csv(os.path.join('intermediate', f'tmp_result_{args.dataset}_at{t}.csv'))

        print('-' * 80)
        print(f'trial {t} is done')
        print(f'  ==> the target model is saved to', target_model_filename)
        print(f'  ==> the surrogate model is saved to', surrogate_model_filename)
        print(f'  ==> the data is saved to', data_filename)
        print(f'  ==> The main result at {t}:')
        print(f'  ', trial_df.groupby(['method', 'num node tokens', 'p']).mean()[['tp', 'tn', 'fp', 'fn']])
        # for method in ['nettack', 'ig', 'ours']:
        #     _df = trial_df[trial_df['method'] == method]
        #     print(f'  ==> Method {method}:')
        #     print(f'    ==> TPR:', (_df['adv prediction'] != _df['prediction']).sum() / len(_df))
        #     # print(f'    ==> FPR:', 1 - _df['adv prediction'].values.sum() / len(_df))
        #     print(f'    ==> TNR:', 1 - _df['incompleteness'].values.sum() / len(_df))
            # print(f'    ==> FNR:', _df['incomplete'].values.sum() / len(_df))
        # print(trial_df.groupby(['trial', 'method', 'num node tokens', 'p']).mean())

        print('-' * 80)

        # for v, perturbations, pred, attack_label in tqdm(zip(candidates, target_nodes_perts, target_preds, attack_labels), desc='verifying', total=len(candidates)):
        #     result['trial'].append(t)
        #     result['target node'].append(v)
        #     result['ground truth'].append(data.y[v].item())
        #     result['prediction'].append(pred)
        #     result['attack_label'].append(attack_label)
        #     result['fragileness'].append(False if perturbations is None else True)

        #     # verification
        #     if perturbations is None:
        #         result['perturbations'].append([])
        #         result['adv prediction'].append(-1)
        #         result['incomplete prediction'].append(-1)
        #         result['incomplete'].append(0)
        #         result['failed perturbation'].append([])
        #         result['single adv predictions'].append([])
        #         continue
        #     result['perturbations'].append(perturbations)

        #     adv_data = copy.deepcopy(data)
        #     edge_index_t = to_undirected(torch.tensor(perturbations).t()) 
        #     adv_data.add_edges(edge_index_t)
        #     adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
        #     adv.train(adv_data, device)
        #     adv_pred = adv.predict(adv_data, device, target_nodes=[v])
        #     result['adv prediction'].append(adv_pred[0])
            
        #     is_incomplete, _failed_pert = check_incompleteness(args, v, pred, adv_pred[0], adv_data, perturbations, device)
        #     if is_incomplete:
        #         result['incomplete prediction'].append(pred)
        #         result['incomplete'].append(1)
        #         result['failed perturbation'].append(_failed_pert)
        #         # double check the perturbations
        #         single_adv_predictions = []
        #         for e in perturbations:
        #             if isinstance(e, list) or isinstance(e, np.ndarray):
        #                 e = tuple(e)
        #             single_adv_pred = _adv_train_predict(args, data, [e], [v], device)[0]
        #             single_adv_predictions.append(single_adv_pred.item())
        #         result['single adv predictions'].append(single_adv_predictions)

        #         print('Yes, it is incompleteness!')
        #         print('  ==> Perturbations:', perturbations)
        #         print('  ==> The failed perturbations:', _failed_pert)
        #         print('  ==> The single adv predictions:', single_adv_predictions)
        #     else:
        #         # print('No, not incompleteness!')
        #         result['incomplete prediction'].append(-1)
        #         result['incomplete'].append(0)
        #         result['failed perturbation'].append([])
        #         result['single adv predictions'].append([])
    try:    
        df = pd.DataFrame(result, index=None)
    except ValueError as err:
        print('result', result)
        raise err
    
    # print(df.groupby(['trial', 'method', 'num node tokens', 'p']).mean())

    # for t in range(args.num_trials):
    #     _df = df[df['trial'] == t]
    #     print(f'Trial {t}, the number of incompleteness:', _df['incomplete'].values.sum())
    #     print(f'Trial {t}, the number of fragile:', _df['fragileness'].values.sum())
    # incompleteness = df['incomplete'].values.sum()
    # print('  ==> The incompleteness:', incompleteness)
    result_filename = _result_filename(args, 'verify')
    df.to_csv(os.path.join('result', result_filename))
    
    print('-' * 80)
    print('Verifing unlearning is done')
    print(f'  ==> the result file is saved to', result_filename)
    # print(f'  ==> the target model is saved to', target_model_filename)
    # print(f'  ==> the surrogate model is saved to', surrogate_model_filename)
    # print(f'  ==> the data is saved to', data_filename)
    print('-' * 80)


def _verify_unlearning(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fixed_seed = True
    else:
        is_fixed_seed = False

    result = defaultdict(list)
    for t in range(args.num_trials):
        logging.info('Start verifing unlearning at trial %d ...', t)
        if not is_fixed_seed:
            args.seed = random.randint(0, 1e+5)
            torch.manual_seed(args.seed)
            torch.cuda.manual_seed_all(args.seed)
            np.random.seed(args.seed)
            random.seed(args.seed)

        data = data_loader.load(args)

        logging.info('  ==> Loading the target model ...')
        target_model = _get_target_model(args, data, device)
        target_train_preds = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist())
        # target_test_preds, target_test_posts = target_model.predict(data, device, target_nodes=data.test_set.nodes.tolist(), return_posterior=True)

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.y = target_train_preds
        # surr_test_preds = surrogate.predict(surrogate_data, device, target_nodes=data.test_set.nodes.tolist())

        logging.info('  ==> Sampling node tokens ...')
        if args.node_sampler == 'boundary':
            candidates = sample_node_tokens(args, data, data.test_set.nodes.tolist(), target_model, amplify=1)
        elif args.node_sampler == 'random':
            candidates = random.sample(data.test_set.nodes.tolist(), args.num_target_nodes)
        target_preds, posteriors = target_model.predict(data, device, target_nodes=candidates, return_posterior=True)
        attack_labels = _second_best_labels(posteriors)

        for method in ['ours', 'nettack', 'minmax']:
            t0 = time.time()
            if method == 'nettack' or method == 'minmax':
                surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True, bias=False, fix_weight=False)
                surrogate.train(data, device)
                surrogate_preds = surrogate.predict(data, device, target_nodes=candidates)

                args.edge_sampler = method
                target_nodes_perts = []
                for v, pred, attack_label in tqdm(zip(candidates, target_preds, attack_labels), total=len(candidates), desc=f'At trial {t}, {method} attacking'):
                    perturbations = sample_edge_tokens(args, data, v, surrogate, pred, target_label=attack_label)
                    target_nodes_perts.append(perturbations)
            elif method == 'ours':
                logging.info('  ==> Loading the surrogate model ...') 
                surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=False, fix_weight=False)
                surrogate.train(data, device)

                logging.info('  ==> start to run CVX searching ...')
                verifier = CertifiedFragileness(args, surrogate, data, device, verbose=False)
                # for c, ta, p in zip(candidates, target_preds.tolist(), posteriors):
                #     certify_fragile_with_perturbations((c, ta, p), args, verifier, data)
                pool = Pool(processes=5)
                target_nodes_perts = list(tqdm(
                    pool.imap(
                        partial(certify_fragile_with_perturbations, args=args,verifier=verifier, data=data),
                        zip(candidates, target_preds.tolist(), posteriors)
                    ),
                    total=len(candidates), desc=f'At trial {t}, certifing'
                ))
                pool.close()
            else:
                raise NotImplementedError('Invalid method:', method)
            total_preparation_time = time.time() - t0

            """ Option 1
                Simulate the behavior of malicious service provider
                that randomly sample p% edges as a subset and do the unlearning or retraining.
            """
            # for num_node_tokens in [10, 20, 30, 40, 50]:
            for num_node_tokens in [50]:
                _candidates, _perturbations, _predictions, _attack_label = [], [], [], []
                for candidate, perturbation, pred, attack_label in zip(candidates[:num_node_tokens], target_nodes_perts[:num_node_tokens], target_preds[:num_node_tokens], attack_labels[:num_node_tokens]):
                    if perturbation is not None and len(perturbation) > 1:
                        _candidates.append(candidate)
                        _perturbations.append(perturbation)
                        _predictions.append(pred)
                        _attack_label.append(attack_label)
                # _candidates = np.array(_candidates)
                # _perturbations = np.array(_perturbations)
                all_perturbations = [p for pert in _perturbations for p in pert]
                _predictions = np.array(_predictions)
                _attack_label = np.array(_attack_label)

                tp, tn, fp, fn = {}, {}, {}, {}
                for p in args.percentages:
                    tp[p], tn[p], fp[p], fn[p] = 0, 0, 0, 0
                    t0 = time.time()
                    for idx in tqdm(range(len(_candidates)), desc=f'At trial {t}, {method} verifying {p}%'):
                        """ Temporarily Add
                            Each trail only has one target node, and randomly sample 1, 2, 3, 4 perturbations as a subset.
                        """

                        # rand_idx = random.choice(range(len(_candidates)))
                        trial_v = _candidates[idx]
                        trial_perts = _perturbations[idx]
                        if len(trial_perts) != 5:
                            logging.warning('The number of perturbations is less than 5, but %d', len(trial_perts))
                        
                        # add perturbations to the graph
                        adv_pred = _adv_train_predict(args, data, trial_perts, [trial_v], device)
                        adv_data = copy.deepcopy(data)
                        adv_data.add_edges(to_undirected(torch.tensor(trial_perts).t()))

                        neg_pred = _unlearn_train_predict(args, adv_data, trial_perts, [trial_v], device)
                        if neg_pred[0] != adv_pred:
                            tn[p] += 1
                        else:
                            fn[p] += 1

                        sub_perts = random.sample(trial_perts, max(int(len(trial_perts) * p), 1))
                        pos_pred = _unlearn_train_predict(args, adv_data, sub_perts, [trial_v], device)
                        if pos_pred[0] == adv_pred:
                            tp[p] += 1
                        else:
                            fp[p] += 1

                        # # server no cheating
                        # neg_preds = _unlearn_train_predict(args, adv_data, all_perturbations, _candidates, device)
                        # if np.sum(neg_preds == adv_predictions) == 0: # all the node tokens label changed
                        #     tn[p] += 1
                        # if np.sum(neg_preds == adv_predictions) > 0: # at least one token did not change
                        #     fp[p] += 1

                        # # server cheating
                        # sub_perts = random.sample(all_perturbations, max(int(len(all_perturbations) * p), 1))
                        # pos_preds = _unlearn_train_predict(args, adv_data, sub_perts, _candidates, device)
                        # if np.sum(pos_preds == adv_predictions) > 0: # at least one token did not change
                        #     tp[p] += 1
                        # if np.sum(pos_preds == adv_predictions) == 0: # all the node tokens label changed
                        #     fp[p] += 1
                        # _result.append(verification_prob.item())
                            

                    verify_time = time.time() - t0
                    result['trial'].append(t)
                    result['method'].append(method)
                    result['total_preparation_time'].append(total_preparation_time)
                    result['num node tokens'].append(num_node_tokens)
                    result['p'].append(p)
                    result['tp'].append(tp[p])
                    result['tn'].append(tn[p])
                    result['fp'].append(fp[p])
                    result['fn'].append(fn[p])    
                    result['verify time'].append(verify_time)

            # print(f'At trial {t}, method {method}, Option 1:')
            # print('  ==>', json.dumps(summary, indent=4))
            # print('=' * 80)

        ts = int(time.time())
        target_model_filename = _target_model_filename(args, f'main{t}_{ts}')
        target_model.save_model(os.path.join('intermediate', target_model_filename))
        surrogate_model_filename = _surrogate_model_filename(args, f'main{t}_{ts}')
        surrogate.save_model(os.path.join('intermediate', surrogate_model_filename))
        data_filename = _data_filename(args, f'main{t}_{ts}')
        with open(os.path.join('intermediate', data_filename), 'wb') as fp:
            pickle.dump(data, fp)
        perturbations_filename = _perturbations_filename(args, f'main{t}_{ts}', method)
        with open(os.path.join('intermediate', perturbations_filename), 'wb') as fp:
            pickle.dump(target_nodes_perts, fp)

        trial_df = pd.DataFrame(result)
        trial_df = trial_df[trial_df['trial'] == t]
        trial_df.to_csv(os.path.join('intermediate', f'tmp_result_{args.dataset}_at{t}.csv'))

        print('-' * 80)
        print(f'trial {t} is done')
        print(f'  ==> the target model is saved to', target_model_filename)
        print(f'  ==> the surrogate model is saved to', surrogate_model_filename)
        print(f'  ==> the data is saved to', data_filename)
        print(f'  ==> The main result at {t}:')
        print(f'  ', trial_df.groupby(['method', 'num node tokens', 'p']).mean()[['tp', 'tn', 'fp', 'fn']])
        # for method in ['nettack', 'ig', 'ours']:
        #     _df = trial_df[trial_df['method'] == method]
        #     print(f'  ==> Method {method}:')
        #     print(f'    ==> TPR:', (_df['adv prediction'] != _df['prediction']).sum() / len(_df))
        #     # print(f'    ==> FPR:', 1 - _df['adv prediction'].values.sum() / len(_df))
        #     print(f'    ==> TNR:', 1 - _df['incompleteness'].values.sum() / len(_df))
            # print(f'    ==> FNR:', _df['incomplete'].values.sum() / len(_df))
        # print(trial_df.groupby(['trial', 'method', 'num node tokens', 'p']).mean())

        print('-' * 80)

        # for v, perturbations, pred, attack_label in tqdm(zip(candidates, target_nodes_perts, target_preds, attack_labels), desc='verifying', total=len(candidates)):
        #     result['trial'].append(t)
        #     result['target node'].append(v)
        #     result['ground truth'].append(data.y[v].item())
        #     result['prediction'].append(pred)
        #     result['attack_label'].append(attack_label)
        #     result['fragileness'].append(False if perturbations is None else True)

        #     # verification
        #     if perturbations is None:
        #         result['perturbations'].append([])
        #         result['adv prediction'].append(-1)
        #         result['incomplete prediction'].append(-1)
        #         result['incomplete'].append(0)
        #         result['failed perturbation'].append([])
        #         result['single adv predictions'].append([])
        #         continue
        #     result['perturbations'].append(perturbations)

        #     adv_data = copy.deepcopy(data)
        #     edge_index_t = to_undirected(torch.tensor(perturbations).t()) 
        #     adv_data.add_edges(edge_index_t)
        #     adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
        #     adv.train(adv_data, device)
        #     adv_pred = adv.predict(adv_data, device, target_nodes=[v])
        #     result['adv prediction'].append(adv_pred[0])
            
        #     is_incomplete, _failed_pert = check_incompleteness(args, v, pred, adv_pred[0], adv_data, perturbations, device)
        #     if is_incomplete:
        #         result['incomplete prediction'].append(pred)
        #         result['incomplete'].append(1)
        #         result['failed perturbation'].append(_failed_pert)
        #         # double check the perturbations
        #         single_adv_predictions = []
        #         for e in perturbations:
        #             if isinstance(e, list) or isinstance(e, np.ndarray):
        #                 e = tuple(e)
        #             single_adv_pred = _adv_train_predict(args, data, [e], [v], device)[0]
        #             single_adv_predictions.append(single_adv_pred.item())
        #         result['single adv predictions'].append(single_adv_predictions)

        #         print('Yes, it is incompleteness!')
        #         print('  ==> Perturbations:', perturbations)
        #         print('  ==> The failed perturbations:', _failed_pert)
        #         print('  ==> The single adv predictions:', single_adv_predictions)
        #     else:
        #         # print('No, not incompleteness!')
        #         result['incomplete prediction'].append(-1)
        #         result['incomplete'].append(0)
        #         result['failed perturbation'].append([])
        #         result['single adv predictions'].append([])
    try:    
        df = pd.DataFrame(result, index=None)
    except ValueError as err:
        print('result', result)
        raise err
    
    # print(df.groupby(['trial', 'method', 'num node tokens', 'p']).mean())

    # for t in range(args.num_trials):
    #     _df = df[df['trial'] == t]
    #     print(f'Trial {t}, the number of incompleteness:', _df['incomplete'].values.sum())
    #     print(f'Trial {t}, the number of fragile:', _df['fragileness'].values.sum())
    # incompleteness = df['incomplete'].values.sum()
    # print('  ==> The incompleteness:', incompleteness)
    result_filename = _result_filename(args, 'verify')
    df.to_csv(os.path.join('result', result_filename))
    
    print('-' * 80)
    print('Verifing unlearning is done')
    print(f'  ==> the result file is saved to', result_filename)
    # print(f'  ==> the target model is saved to', target_model_filename)
    # print(f'  ==> the surrogate model is saved to', surrogate_model_filename)
    # print(f'  ==> the data is saved to', data_filename)
    print('-' * 80)




def global_attack_for_1perturbation(args):
    device = utils.get_device(args)
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fix_seed = True
    else:
        is_fix_seed = False
 
    result = defaultdict(list)
    for _ in range(args.num_trials):
        if not is_fix_seed:
            args.seed = random.randint(0, 1e+5)
            random.seed(args.seed)
            np.random.seed(args.seed)
            torch.manual_seed(args.seed)
        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device)
        target_train_preds, target_posts = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist(), return_posterior=True)
        target_test_preds, target_test_posts = target_model.predict(data, device, target_nodes=data.test_set.nodes.tolist(), return_posterior=True)
        test_nodes2preds = {v: pred for v, pred in zip(data.test_set.nodes.tolist(), target_test_preds)}

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.y = target_train_preds
        if args.edge_sampler == 'nettack':
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True, bias=False)
        else:
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True)
        surrogate.train(surrogate_data, device)
        surr_test_preds = surrogate.predict(surrogate_data, device, target_nodes=data.test_set.nodes.tolist())

        attacker = Minimum_MinMax_Attack(surrogate, data, budget=10)
        adj = data.adjacency_matrix()
        perturbations = attacker.attack(data.x, adj, data.y, data.train_set.nodes)
        if perturbations is None or len(perturbations) == 0:
            print('No perturbations found.')
            exit(0)
        print('perturbations:', perturbations)

        adv_data = copy.deepcopy(data)
        adv_data.add_edges(to_undirected(torch.tensor(perturbations).t()))
        adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
        adv.train(adv_data, device)
        adv_pred = adv.predict(adv_data, device, target_nodes=data.test_set.nodes.tolist())

        print((surr_test_preds != adv_pred).sum())


def estimate_pm_on_1pertubation_fragile_nodes(args):
    device = utils.get_device(args)
    # random generate a seed for each trial if args.seed is None
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fix_seed = True
    else:
        is_fix_seed = False
 
    result = defaultdict(list)
    for _ in range(args.num_trials):
        if not is_fix_seed:
            args.seed = random.randint(0, 1e+5)
            random.seed(args.seed)
            np.random.seed(args.seed)
            torch.manual_seed(args.seed)
        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device)
        target_train_preds, target_posts = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist(), return_posterior=True)
        target_test_preds, target_test_posts = target_model.predict(data, device, target_nodes=data.test_set.nodes.tolist(), return_posterior=True)
        test_nodes2preds = {v: pred for v, pred in zip(data.test_set.nodes.tolist(), target_test_preds)}

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.y = target_train_preds
        if args.edge_sampler == 'nettack':
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True, bias=False)
        else:
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, surrogate=True)
        surrogate.train(surrogate_data, device)

        one_pert_fragile_candidates = {}
        t0 = time.time()
        if args.find_fragile_method == 'certify':
            one_pert_fragile_candidates = find_non_robust_nodes(args, surrogate, data, data.test_set.nodes.tolist())
        elif args.find_fragile_method == 'margin':
            predictions, _ = surrogate.predict(data, device, target_nodes=data.test_set.nodes.tolist(), return_posterior=True)
            one_pert_fragile_candidates = find_1perturbation_fragile_nodes(surrogate.model, data, data.test_set.nodes.tolist(), predictions, device)
        elif args.find_fragile_method == 'boundary':
            near_boundary_nodes = {}
            for idx, p in enumerate(target_test_posts):
                # if utils.near_boundary(z, args.k):
                near_boundary_nodes[data.test_set.nodes.tolist()[idx]] = utils.boundary_score(p)
            sorted_boundary_nodes = {k: v for k,v in sorted(near_boundary_nodes.items(), key=lambda item: item[1])}
            one_pert_fragile_candidates = list(sorted_boundary_nodes.keys())[:369]
        else:
            for v in tqdm(data.test_set.nodes.tolist(), desc=f'  find 1-perturbation fragile nodes'):
                perturbations = sample_edge_tokens(args, data, v, surrogate, test_nodes2preds[v], data.y[v])
                if perturbations is None or len(perturbations) == 0:
                    continue
                one_pert_fragile_candidates[v] = perturbations
        fragile_time = int(time.time() - t0)
        print(f'Number of 1-perturbation fragile candidates: {len(one_pert_fragile_candidates)}')
        print(f' The running time: {fragile_time}s')

        one_pert_fragile_nodes = {}
        one_pert_fragile_nodes2pred = {}
        t0 = time.time()
        if args.skip_fragile_check:
            for v, perturbations in one_pert_fragile_candidates.items():
                one_pert_fragile_nodes[v] = perturbations
                one_pert_fragile_nodes2pred[v] = -1
        else:
            if args.find_fragile_method == args.find_pert_method:
                for v, perturbations in tqdm(one_pert_fragile_candidates.items(), desc=f'  verify 1-perturbation fragile nodes', total=len(one_pert_fragile_candidates)):
                    verification_edges, verification_preds = _verify_1perturbation_fragile(data, v, test_nodes2preds[v], perturbations, device)
                    if len(verification_edges) > 0:
                        value, indices, c = np.unique(verification_preds, return_inverse=True, return_counts=True)
                        if len(value) > 1:
                            filter_indices = np.where(indices == np.argmax(c))[0]
                            one_pert_fragile_nodes[v] = [e for i, e in enumerate(verification_edges) if i in filter_indices]
                            one_pert_fragile_nodes2pred[v] = value[np.argmax(c)]
                        else:
                            one_pert_fragile_nodes[v] = verification_edges
                            one_pert_fragile_nodes2pred[v] = value[0]
            else:
                args.edge_sampler = args.find_pert_method
                for v in tqdm(one_pert_fragile_candidates, 'verify 1-perturbation fragile nodes'):
                    perturbations = sample_edge_tokens(args, data, v, surrogate, test_nodes2preds[v], data.y[v])
                    if perturbations is None or len(perturbations) == 0:
                        continue
                    verification_edges, verification_preds = _verify_1perturbation_fragile(data, v, test_nodes2preds[v], perturbations, device)
                    if len(verification_edges) > 0:
                        value, indices, c = np.unique(verification_preds, return_inverse=True, return_counts=True)
                        if len(value) > 1:
                            filter_indices = np.where(indices == np.argmax(c))[0]
                            one_pert_fragile_nodes[v] = [e for i, e in enumerate(verification_edges) if i in filter_indices]
                            one_pert_fragile_nodes2pred[v] = value[np.argmax(c)]
                        else:
                            one_pert_fragile_nodes[v] = verification_edges
                            one_pert_fragile_nodes2pred[v] = value[0]
        
        print(f'Verification time: {int(time.time() - t0)}s')
        print(f'Number of verified 1-perturbation fragile nodes: {len(one_pert_fragile_nodes)}.')
        print(f'  The successful rate is {(len(one_pert_fragile_nodes) / len(data.test_set.nodes)):.4f}')
        # print('  ', one_pert_fragile_nodes)

        # for v, perturbations in tqdm(one_pert_fragile_nodes.items(), desc='  estimating pm', total=len(one_pert_fragile_nodes)): 
        for v, perturbations in one_pert_fragile_nodes.items(): 
            # clean_pred = target_model.predict(data, device, target_nodes=[v])
            print(f'Checking node {v} (clean prediction: {test_nodes2preds[v]}), with {len(perturbations)} perturbations...')

            adv_data = copy.deepcopy(data)
            adv_data.add_edges(to_undirected(torch.tensor(perturbations).t()))
            adv = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
            adv.train(adv_data, device)
            adv_pred = adv.predict(adv_data, device, target_nodes=[v])

            is_incompleteness = check_incompleteness(args, v, test_nodes2preds[v], adv_pred[0], adv_data, perturbations, device)
            result['target'].append(v)
            result['label'].append(data.y[v].item())
            result['clean prediction'].append(test_nodes2preds[v])
            result['adv prediction'].append(adv_pred[0])
            result['perturbations'].append(perturbations)
            result['single perturbations'].append(one_pert_fragile_nodes2pred[v])
            result['is incompleteness'].append(1 if is_incompleteness else 0)
            print(f'  {v} imcompleteness: {is_incompleteness}')
            print()

        df = pd.DataFrame(result)
        result_filename = _result_filename(args, '1pert_fragile')
        df.to_csv(os.path.join('result', result_filename))

        pm = df['is incompleteness'].sum() / len(df) 
        print('p_m:', pm)
        print('Verifing the 1-perturbation fragility is Done, the result file is saved to', result_filename)


def measure_asr(args):
    device = utils.get_device(args)
    # random generate a seed for each trial if args.seed is None
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fix_seed = True
    else:
        is_fix_seed = False

    asr_list = []
    result = defaultdict(list)
    for _ in range(args.num_trials):
        if not is_fix_seed:
            args.seed = random.randint(-1e+5, 1e+5)
        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device)
        target_preds, target_post = target_model.predict(
            data, device, 
            target_nodes=data.train_set.nodes.tolist(), 
            return_posterior=True
        )
        node_tokens = sample_node_tokens(
            args, data, 
            data.train_set.nodes.tolist(), 
            posterior=target_post,
            amplify=1
        )

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.labels = target_preds

        if args.edge_sampler == 'nettack':
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, bias=False)
        else:
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes)
        surrogate.train(surrogate_data, device)
        attack_success_count = 0
        for target_node in tqdm(node_tokens, desc='asr'):
            if args.verbose:
                print(f'Attack {target_node} (degree: {data.degree(target_node)})')
            clean_pred = target_model.predict(data, device, target_nodes=[target_node])

            pred, surr_logit = surrogate.predict(data, device, target_nodes=[target_node], return_logit=True)
            triggers = sample_edge_tokens(args, data, target_node, surrogate, clean_pred[0], data.y[target_node])
            # if args.verbose:
            print('E_t:', triggers)
            if triggers is None or len(triggers) == 0:
                continue
            edge_index_t = to_undirected(torch.tensor(triggers).t()) 
            adv_data = copy.deepcopy(data)
            adv_data.add_edges(edge_index_t)
            adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
            adv_model.train(adv_data, device)
            adv_pred = adv_model.predict(adv_data, device, target_nodes=[target_node])

            if adv_pred[0] != clean_pred[0]:
                attack_success_count += 1

            result['clean prediction'].append(clean_pred[0])
            result['adv prediction'].append(adv_pred[0])
            result['surr prediction'].append(pred[0])
            result['perturbations'].append(triggers)
            # result['asr'].append(attack_success_count / len(node_tokens))
        asr = attack_success_count / len(node_tokens)
        asr_list.append(asr)

 
    df = pd.DataFrame(data=result)
    print('=' * 25, 'RESULT', '=' * 25)
    print('  ', np.mean(asr_list))
    print('-' * 58)
    result_path = _result_filename(args, 'asr')
    df.to_csv(os.path.join('result', result_path))
    print('Measuring ASR is Done, the result file is saved to', result_path)

def estimate_q(args):
    device = utils.get_device(args)

    # random generate a seed for each trial if args.seed is None
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        is_fix_seed = True
    else:
        is_fix_seed = False

    clean_acc, adv_acc = [], []
    result = defaultdict(list)
    for _ in range(args.num_trials):
        if not is_fix_seed:
            args.seed = random.randint(-1e+5, 1e+5)
        data = data_loader.load(args)

        # suppose the verifier has the full access of dataset
        target_model = _get_target_model(args, data, device)
        target_result = target_model.evaluate(data, device)
        clean_acc.append(target_result['accuracy'])
        target_preds, target_posterior = target_model.predict(data, device, target_nodes=data.train_set.nodes.tolist(), return_posterior=True)
        
        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.labels = target_preds
        if args.edge_sampler == 'nettack':
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes, bias=False)
        else:
            surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes)
        surrogate.train(surrogate_data, device)

        node_tokens = sample_node_tokens(
            args, data, 
            data.train_set.nodes.tolist(), 
            surrogate,
            posterior=target_posterior,
            amplify=10,
        )
        if args.batch_attack:
            pass
            # clean_model = GNN(args, data.num_features, data.num_classes, surrogate=False, fix_weight=True)
            # clean_model.train(data, device)
            # clean_preds = clean_model.predict(data, device, target_nodes=target_nodes)
            # triggers = sample_edge_tokens_batch(args, data, target_nodes, _surrogate_model, clean_preds)
            # print('-' * 50)
            # print('triggers', triggers)
            # print('# of triggers:', len(triggers))
            # print('-' * 50)
            # exit(0)
        else:
            attack_success_count = 0
            tried_num_node_tokens = 0
            # for target_node in tqdm(node_tokens, desc='calculating q'):
            with tqdm(total=args.num_target_nodes, disable=True) as pbar:
                while len(result['target node']) < args.num_target_nodes:
                # while len(result['target node']) <= len(node_tokens):
                    if len(node_tokens) == 0:
                        break

                    if isinstance(node_tokens, dict):
                        target_node, target_labels = node_tokens.popitem()
                        target_label = target_labels[0]
                    else:
                        target_node = node_tokens.pop(0)
                        target_label = None
                    # print('Attacking', target_node, '...')
                    clean_pred = target_model.predict(data, device, target_nodes=[target_node])
                    pred = surrogate.predict(surrogate_data, device, target_nodes=[target_node])

                    if args.edge_sampler == 'mibtack':
                        for _ in range(3):
                            perturbations = sample_edge_tokens(args, data, target_node, surrogate, clean_pred[0])
                            print('perturbations:', perturbations)
                    else: 
                        perturbations = sample_edge_tokens(args, data, target_node, surrogate, clean_pred[0], target_label=target_label)
                    # if args.verbose:
                    # print('E_t:', perturbations)
                    tried_num_node_tokens += 1
                    if perturbations is None or len(perturbations) == 0:
                        continue
                    edge_index_t = to_undirected(torch.tensor(perturbations).t()) 
                    adv_data = copy.deepcopy(data)
                    adv_data.add_edges(edge_index_t)
                    adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False, fix_weight=True)
                    adv_model.train(adv_data, device)
                    adv_result = adv_model.evaluate(adv_data, device)
                    adv_acc.append(adv_result['accuracy'])
                    adv_pred = adv_model.predict(adv_data, device, target_nodes=[target_node])

                    if adv_pred[0] == clean_pred[0]:
                        print('Attack failed, continue to next node token.')
                        continue
                    else:
                        attack_success_count += 1
                    
                    if len(perturbations) >= 10:
                        print(f'Too many triggers ({len(perturbations)}), continue to next node token.')
                        continue
                    
                    print(f'Node token ({target_node}), perturbations: {perturbations}, clean pred: {clean_pred[0].item()}, adv pred: {adv_pred[0].item()}.')
                    if len(perturbations) <= 1:
                        print('Only has one trigger, cannot measure incompleteness, continue to next node token.')
                        # result['sub adv prediction'].append(adv_pred[0])
                        continue

                    if args.check_incompleteness: # counting the incompleteness
                        if is_incompleteness(args, target_node, clean_pred[0], adv_pred[0], adv_data, perturbations, device):
                            print('Yes, it is incompleteness!')
                            result['sub adv prediction'].append(clean_pred[0])
                        else:
                            print('No, not incompleteness!')
                            result['sub adv prediction'].append(adv_pred[0])

                    # counting the Incorrectness of Adding noise (IN)
                    if args.check_incorrectness:
                        for sigma in args.sigma:
                            noise_model = copy.deepcopy(adv_model)
                            parameters = [p for p in noise_model.parameters() if p.requires_grad]
                            noise = [torch.randn_like(p) * sigma for p in parameters]
                            with torch.no_grad():
                                delta = [p + infl for p, infl in zip(parameters, noise)]
                                for i, p in enumerate(parameters):
                                    p.copy_(delta[i])
                            noise_adv_pred = noise_model.predict(data, device, target_nodes=[target_node])
                            result[f'N@{sigma} prediction'].append(noise_adv_pred[0])

                        # counting the Incorrectness of insufficient of training (II) 
                        insufficient_model = GNN(args, data.num_features, data.num_classes, surrogate=False)
                        intermediate, E = insufficient_model.insufficient_train(data, device)
                        insufficient_model.model.load_state_dict(intermediate[int(E / 2)])
                        insufficient_pred = insufficient_model.predict(data, device, target_nodes=[target_node])
                        result[f'I prediction'].append(insufficient_pred[0])

                    result['target node'].append(target_node)
                    result['adv edges'].append(perturbations)
                    result['# of adv edges'].append(len(perturbations))
                    # result['# of combinations'].append(num_of_c)
                    # result['# of flip'].append(num_of_flip)
                    result['surrogate prediction'].append(pred[0])
                    result['adv prediction'].append(adv_pred[0])
                    result['clean prediction'].append(clean_pred[0])
                    result['ground truth'].append(data.y[target_node])
                    # result['detail'].append(dict(statistic))
                    # pbar.update(1)
                    # pbar.set_description(f'have tried {tried_num_node_tokens}')
                    print()

    # print('clean accuracy', np.mean(clean_acc))
    # print('adv accuracy', np.mean(adv_acc))
    # print('-' * 20, 'ASR', '-' * 20)
    # print('  ', attack_success_count/tried_num_node_tokens)
    # print('-' * 45)

    df = pd.DataFrame(result)
    # _theta =  args.theta if args.theta > 0 else ''
    # result_path = os.path.join(
    #     'result', 
    #     f'setting1_q_{args.dataset}_{args.node_sampler}{_theta}_{args.num_target_nodes}_{args.edge_sampler}'
    # )
    # if args.epsilon >= 0:
    #     result_path += f'_epsilon{args.epsilon}'

    # timestamp = int(time.time())
    # result_path += f'_{timestamp}.csv'
    result_name = _result_filename(args, 'q')
    df.to_csv(os.path.join('result', result_name))
    print('Finished, save the result to', result_name)
    return df

def verify(args):
    device = utils.get_device(args)

    result = defaultdict(list)
    for t in range(args.num_trials):
        data = data_loader.load(args)

        target_model = _get_target_model(args, data, device=device)
        target_result = target_model.evaluate(data, device) 
        target_preds, target_post = target_model.predict(
            data, device,
            target_nodes=data.train_set.nodes.tolist(),
            return_posterior=True
        )
        node_tokens = sample_node_tokens(args, data, data.train_set.nodes.tolist(), posterior=target_post)

        surrogate_data = copy.deepcopy(data)
        surrogate_data.train_set.labels = target_preds
        surrogate = GNN(args, surrogate_data.num_features, surrogate_data.num_classes)
        surrogate.train(surrogate_data, device)

        attack_success_count = 0
        result = defaultdict(list)
        for target_node in tqdm(node_tokens, desc='verifing'):
            clean_pred = target_model.predict(data, device, target_nodes=[target_node], return_posterior=True)

            perturbations = sample_edge_tokens(args, data, target_node, surrogate, clean_pred[0])
            print('perturbation:', perturbations)
            if perturbations is None or len(perturbations) == 0:
                continue
                
            attack_success_count += 1

            edge_index_t = to_undirected(torch.tensor(perturbations).t())                
            adv_data = copy.deepcopy(data)
            adv_data.add_edges(edge_index_t)
            adv_model = GNN(args, adv_data.num_features, adv_data.num_classes, surrogate=False)
            adv_model.train(adv_data, device)
            adv_pred = adv_model.predict(adv_data, device, target_nodes=[target_node])
            adv_result = adv_model.evaluate(data, device)

            result['target node'].append(target_node)
            result['adv edges'].append(perturbations)
            result['# of adv edges'].append(len(perturbations))
            result['clean acc'].append(target_result['accuracy'])
            result['adv acc'].append(adv_result['accuracy'])
            result['success'].append(1 if adv_pred[0] != clean_pred[0] else 0)

            # num_of_c = sum([math.comb(len(E_t), p) for p in range(1, len(E_t))])
            # num_of_flip = 0
            # statistic = defaultdict(int)
            # for length in range(1, len(E_t)):
            #     for _edges in combinations(E_t, length):
            #         sub_adv_nodes = [v for _, v in _edges]
            #         sub_adv_data = copy.deepcopy(adv_data)
            #         sub_adv_data.remove_edges(_edges)
            #         # print(adv_data.edge_index.size(1), '-', len(_edges), '=', sub_adv_data.edge_index.size(1))
            #         sub_adv_model = GNN(args, sub_adv_data.num_features, sub_adv_data.num_classes, surrogate=False)
            #         sub_adv_model.train(sub_adv_data, device)
            #         sub_adv_pred, _,  sub_adv_post = sub_adv_model.predict(sub_adv_data, device, target_nodes=[target_node], return_posterior=True)
            #         if adv_pred[0] != sub_adv_pred[0]:
            #             num_of_flip += 1
            #             statistic[len(_edges)] += 1
            if attack_success_count == args.num_target_nodes - 1:
                break
 
    df = pd.DataFrame(result)
    result_name = _result_filename(args, 'verify')
    df.to_csv(os.path.join('result', result_name))
    print('Finished, save the result to', result_name)
    return df


def calc_escape_prob_by_trials(df, num_per_trial, q):
    p_e = []
    for i in range(0, len(df), num_per_trial):
        _df = df[i: i+num_per_trial]
        K = np.sum(_df['success'].values)
        K_t = len(_df)
        p_e.append(_escape_probability(K, K_t, q))
    return np.mean(p_e)

logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s %(name)s:%(lineno)d - %(levelname)s - %(message)s',
)

if __name__ == '__main__':

    parser = argument.load_parser()
    parser.add_argument('--task', type=str, required=True, 
                        help='Calculate q or verify the removal probability, [q|verify].')
    parser.add_argument('--baseline', action='store_true')
    parser.add_argument('--backdoor', action='store_true')

    parser.add_argument('--num-target-nodes', dest='num_target_nodes', type=int, default=50)
    parser.add_argument('--node-sampler', dest='node_sampler', type=str, default='random', 
                        help='How to sample target nodes, distance|boundary')
    parser.add_argument('--theta', type=float, default=0.)
    # parser.add_argument('--epsilon', type=float, default=-1)
    parser.add_argument('--edge-sampler', dest='edge_sampler', type=str, default='nettack')
    parser.add_argument('--sigma', type=float, nargs='+', default=[0.01, 0.1, 1, 10])
    parser.add_argument('--check-incompleteness', action='store_true')
    parser.add_argument('--check-incorrectness', action='store_true')
    parser.add_argument('--batch-attack', action='store_true')
    
    parser.add_argument('--candidate-method', default='label', type=str)
    parser.add_argument('--candidate-size', default=10, type=int)
    parser.add_argument('--candidate-hop', type=int, default=2)

    parser.add_argument('--num-perts', dest='num_perts', type=int, default=5)
    parser.add_argument('--percentages', type=float, nargs='+', default=[0.1, 0.2, 0.3, 0.4, 0.5])


    args = parser.parse_args()
    
    mp.set_start_method('spawn')

    if args.task == 'q': 
        # find_certified_nodes(args)
        # estimate q of setting1
        df_q = estimate_q(args)
        asr = utils.calc_asr_by_trials(df_q, args.num_target_nodes)
        print('#' * 50)
        print('                    ASR:', asr)
        if args.check_incompleteness:
            q = utils.calc_q_by_trials(df_q, args.num_target_nodes)
            print('         Incompleteness:', q) 
        if args.check_incorrectness:
            for s in args.sigma:
                q_n = utils.calc_incorrectness_noise_by_trials(df_q, args.num_target_nodes, sigma=s)
                print(f'  Incorrectness@NI@{s}:', q_n)
            ii = utils.calc_incorrectness_ii_by_trials(df_q, args.num_target_nodes)
            print('       Incorrectness@II:', ii)
            print('-' * 50)
        
    elif args.task == 'verify':
        if args.baseline:
            # for attack in ['pgd', 'ig', 'gf', 'minmax']:
            for attack in ['nettack']:
                args.edge_sampler = attack
                print('-' * 80)
                print('Attack:', attack)
                print('-' * 80)
                verify_unlearning_baselines(args)
        elif args.backdoor:
            verify_unlearning_backdoor(args)
        else:
            verify_unlearning(args)
        # First, estimate the escape probability (S cheats on unlearning)
        # df_q = estimate_q(args)
        # q = utils.calc_q_by_trials(df_q, args.num_target_nodes)
        # verify the removal probability
        # df_verify = verify(args)
        # p_e = calc_escape_prob_by_trials(df_verify, args.num_target_nodes, q)
        # asr = utils.calc_asr_by_trials(df_verify, args.num_target_nodes) 
        # print('#' * 25, 'RESULT', '#' * 25)
        # print('  clean acc:', df_verify['clean acc'].mean())
        # print('    adv acc:', df_verify['adv acc'].mean())
        # print('          q:', q)
        # print('        p_e:', p_e)
        # print('          P:', 1 - p_e)
        # print('=' * 55)
        # mean_q = np.mean(list(node_token2q.values()))
        # p = utils.calc_removal_probability(df_verify, mean_q, args.num_trials)
        # max_q = np.max(list(node_token2q.values()))
        # max_p = utils.calc_removal_probability(df_verify, max_q, args.num_trials)
    elif args.task == 'main':
        _verify_unlearning(args)
    elif args.task == 'asr':
        measure_asr(args)
        # for attack in ['nettack', 'pgd', 'ig', 'gf', 'minmax']:
        #     args.edge_sampler = attack
        #     args.seed = None
        #     measure_asr(args)

    elif args.task == 'compare':
        compare_node_samplers(args)
    elif args.task == 'fragile':
        verify_1perturbation_fragile_nodes(args)
    elif args.task == 'fragile-pm':
        estimate_pm_on_1pertubation_fragile_nodes(args)
    elif args.task == 'global':
        global_attack_for_1perturbation(args)